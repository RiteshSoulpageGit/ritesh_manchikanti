{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_url(word):\n",
    "    # Regular expression pattern to match URL patterns\n",
    "    url_pattern = re.compile(r\"(?:^|\\s)((?:www\\.|(?:https?://|ftp://)\\S+?)(\\w+.\\.(?:com|in|uk\\.org))\\b)\")\n",
    "    return bool(url_pattern.match(word))\n",
    "\n",
    "\n",
    "def spell_check_document(pdf_path):\n",
    "    spell = SpellChecker()\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Tokenize the text using special characters\n",
    "    words = word_tokenize(document_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Exclude URLs from spell checking\n",
    "    words = [word for word in words if not is_url(word) and word.isalpha()]\n",
    "    \n",
    "    misspelled_words = spell.unknown(words)\n",
    "    \n",
    "    if len(misspelled_words) > 0:\n",
    "        print(\"Misspelled words:\")\n",
    "        for misspelled_word in misspelled_words:\n",
    "            print(misspelled_word)\n",
    "    else:\n",
    "        print(\"No misspelled words found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "spell_check_document(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_grammatical_errors(pdf_path):\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    language = detect(document_text)\n",
    "    \n",
    "    # Initialize LanguageTool for the detected language\n",
    "    tool = LanguageTool(language)\n",
    "    \n",
    "    # Perform grammatical error detection\n",
    "    errors = tool.check(document_text)\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(\"Grammatical errors:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No grammatical errors found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "detect_grammatical_errors(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i have a dog as pet.', 'i have dog as pet.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=164, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=164,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "## Testing the model on text\n",
    "text = '''\n",
    "i have dog as pet  \n",
    " '''\n",
    "\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def load_model():\n",
    "    model_name = \"bert-base-uncased\"  # Replace with the appropriate pre-trained model name\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    return model, tokenizer\n",
    "\n",
    "def classify_grammar(sentence, model, tokenizer):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class_index = torch.argmax(probabilities).item()\n",
    "    label_mapping = {0: \"Grammatically Correct\", 1: \"Grammatically Incorrect\"}\n",
    "    predicted_class_label = label_mapping[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Example usage\n",
    "sentence = \"She don't likes ice cream..\"\n",
    "\n",
    "# \"She don't likes ice cream.\"\n",
    "# \"He plays the piano and tennis.\"\n",
    "# \"My dogs eat cookies and eats cake.\"\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "predicted_label = classify_grammar(sentence, model, tokenizer)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
