{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_url(word):\n",
    "    # Regular expression pattern to match URL patterns\n",
    "    url_pattern = re.compile(r\"(?:^|\\s)((?:www\\.|(?:https?://|ftp://)\\S+?)(\\w+.\\.(?:com|in|uk\\.org))\\b)\")\n",
    "    return bool(url_pattern.match(word))\n",
    "\n",
    "\n",
    "def spell_check_document(pdf_path):\n",
    "    spell = SpellChecker()\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Tokenize the text using special characters\n",
    "    words = word_tokenize(document_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Exclude URLs from spell checking\n",
    "    words = [word for word in words if not is_url(word) and word.isalpha()]\n",
    "    \n",
    "    misspelled_words = spell.unknown(words)\n",
    "    \n",
    "    if len(misspelled_words) > 0:\n",
    "        print(\"Misspelled words:\")\n",
    "        for misspelled_word in misspelled_words:\n",
    "            print(misspelled_word)\n",
    "    else:\n",
    "        print(\"No misspelled words found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "spell_check_document(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_grammatical_errors(pdf_path):\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    language = detect(document_text)\n",
    "    \n",
    "    # Initialize LanguageTool for the detected language\n",
    "    tool = LanguageTool(language)\n",
    "    \n",
    "    # Perform grammatical error detection\n",
    "    errors = tool.check(document_text)\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(\"Grammatical errors:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No grammatical errors found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "detect_grammatical_errors(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
