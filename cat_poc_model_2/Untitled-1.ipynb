{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_url(word):\n",
    "    # Regular expression pattern to match URL patterns\n",
    "    url_pattern = re.compile(r\"(?:^|\\s)((?:www\\.|(?:https?://|ftp://)\\S+?)(\\w+.\\.(?:com|in|uk\\.org))\\b)\")\n",
    "    return bool(url_pattern.match(word))\n",
    "\n",
    "\n",
    "def spell_check_document(pdf_path):\n",
    "    spell = SpellChecker()\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Tokenize the text using special characters\n",
    "    words = word_tokenize(document_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Exclude URLs from spell checking\n",
    "    words = [word for word in words if not is_url(word) and word.isalpha()]\n",
    "    \n",
    "    misspelled_words = spell.unknown(words)\n",
    "    \n",
    "    if len(misspelled_words) > 0:\n",
    "        print(\"Misspelled words:\")\n",
    "        for misspelled_word in misspelled_words:\n",
    "            print(misspelled_word)\n",
    "    else:\n",
    "        print(\"No misspelled words found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "spell_check_document(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_grammatical_errors(pdf_path):\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "    language = detect(document_text)\n",
    "    \n",
    "    # Initialize LanguageTool for the detected language\n",
    "    tool = LanguageTool(language)\n",
    "    \n",
    "    # Perform grammatical error detection\n",
    "    errors = tool.check(document_text)\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(\"Grammatical errors:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No grammatical errors found.\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "detect_grammatical_errors(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "\n",
    "import io\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from langdetect import detect\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_chunks(text, chunk_size, prompt):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    nlp = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    generated_texts = []\n",
    "    for chunk in chunks:\n",
    "        input_text = prompt + chunk\n",
    "        generated_text = nlp(input_text, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "        generated_texts.append(generated_text)\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './DATA/book.pdf'\n",
    "chunk_size = 500\n",
    "prompt = \"Please generate a response for the following text:\\n\\n\"\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "generated_texts = process_chunks(text, chunk_size, prompt)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=164, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=164,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "## Testing the model on text\n",
    "text = ''' I want to dedicate this book to my family. Rachel, Connor, and Ben, you are my life. To my \n",
    "wonderful mother, Susan, and my sister, Kristy, - thank you for the love and support. I also \n",
    "would like to thank the faculty, staff, and students at both Notre Dame of Maryland \n",
    "University and Johns Hopkins University for allowing me to fulfill my passion for teaching \n",
    "the teachers of today and tomorrow. -Ryan L. Schaaf \n",
    " \n",
    "I want to dedicate this book to my parents, who always promoted the value of a strong \n",
    "education. From installing my pretend classroom in our basement and pretending to be my \n",
    "first students, to encouraging me throughout college and graduate school, I am forever \n",
    "grateful. To my husband, Nick, thank you for being my first editor and always allowing me to \n",
    "bounce ideas off of you. We are a team. Thank you for always supporting me. Ian and Ryan, \n",
    "thank you for this unbelievable opportunity and all that you have taught me throughout this \n",
    "experience. -Becky Zayas \n",
    " \n",
    "This book is intended to celebrate the exceptional dedication and courage educators have \n",
    "exhibited, and to acknowledge their demonstrated capacity to adapt and innovate in \n",
    "extraordinarily challenging and uncertain conditions. Now is the time for us to recognize the \n",
    "exceptional role they play, and to empower them with the training, professional development, \n",
    "support, and working conditions needed to effectively deploy their talents. For the education \n",
    "system to recover from the COVID pandemic requires sustained investment in the well-\n",
    "being, training, professional development and working conditions of the worldâ€™s 71 million \n",
    "educators. Education recovery will only be successful if it is conducted hand-in-hand with \n",
    "teachers, giving them both voice and agency to participate in the critical change process. - Ian \n",
    "Jukes \n",
    "    \n",
    " '''\n",
    "\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def load_model():\n",
    "    model_name = \"bert-base-uncased\"  # Replace with the appropriate pre-trained model name\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    return model, tokenizer\n",
    "\n",
    "def classify_grammar(sentence, model, tokenizer):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class_index = torch.argmax(probabilities).item()\n",
    "    label_mapping = {0: \"Grammatically Correct\", 1: \"Grammatically Incorrect\"}\n",
    "    predicted_class_label = label_mapping[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Example usage\n",
    "sentence = \"She don't likes ice cream..\"\n",
    "\n",
    "# \"She don't likes ice cream.\"\n",
    "# \"He plays the piano and tennis.\"\n",
    "# \"My dogs eat cookies and eats cake.\"\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "predicted_label = classify_grammar(sentence, model, tokenizer)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
