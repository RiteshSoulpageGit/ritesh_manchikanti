{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:59:26.928860: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-20 13:59:26.984370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 13:59:27.708169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm , trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import nltk\n",
    "# nltk.download('punkt')  # Download the punkt tokenizer for sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the saved state dictionary into the model\n",
    "model_file = '/home/ubuntu/ritesh_manchikanti/cat_poc_model_2/bert-based-uncased-GED.pth'\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_first_sentence_prediction(model,tokenizer, sentences):\n",
    "    # Load the BERT model and tokenizer\n",
    "    # model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # We need to add special tokens at the beginning and end of each sentence\n",
    "    # for BERT to work properly\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "    labels = [0]  # Dummy labels for prediction, you can ignore this since we only predict the first sentence.\n",
    "\n",
    "    # Tokenize and pad the sentences\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    MAX_LEN = 128\n",
    "    input_ids = pad_sequences(\n",
    "        [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "        maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "    )\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert inputs to tensors\n",
    "    prediction_inputs = torch.tensor(input_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    prediction_labels = torch.tensor(labels)\n",
    "\n",
    "    # Set device to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = model(prediction_inputs.to(device), token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions = np.argmax(logits, axis=1).flatten()\n",
    "\n",
    "    return predictions[0]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# # Example usage\n",
    "# sentences = [\"They drank the pub.\", \"She likes ice cream.\"]\n",
    "# predicted_label = get_first_sentence_prediction(sentences)\n",
    "# print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 in Chunk 1: Sentence Label: incorrect, Learner Choice, Learning Voice \n",
      "\n",
      "Learner Voice, Learner Choice offers fresh, forward-thinking supports for teachers creating \n",
      "an empowered, student-centered classroom.\n",
      "Sentence 2 in Chunk 1: Sentence Label: correct, Learner agency is a major topic in todayâ€™s \n",
      "schools, but what does it mean in practice, and how do these practices give students skills and \n",
      "opportunities they will need to thrive as citizens, parents, and workers in our ever-shifting \n",
      "climate?\n",
      "Sentence 3 in Chunk 1: Sentence Label: incorrect, Showcasing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "# Function to divide text into chunks of 512 tokens\n",
    "def divide_text_into_chunks(text, max_chunk_size=512):\n",
    "    tokenized_texts = text.split()  # Assuming the text is already tokenized by spaces, adjust as needed\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for tokenized_text in tokenized_texts:\n",
    "        if len(current_chunk) + len(tokenized_text) <= max_chunk_size:\n",
    "            current_chunk += tokenized_text + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = tokenized_text + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "text_splitter_1 = TokenTextSplitter(chunk_size=100,chunk_overlap = 2)\n",
    "# Example usage\n",
    "pdf_path = './15031-4983-FullBook.pdf'\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = text_splitter_1.split_text(text)\n",
    "# chunks = divide_text_into_chunks(text)\n",
    "# Predict each chunk\n",
    "for idx, chunk in enumerate(chunks[:1]):\n",
    "    sentences = sent_tokenize(chunk)\n",
    "    label_mapping = {0: \"incorrect\", 1: \"correct\"}\n",
    "    for sentence in sentences:\n",
    "        sentence_label = get_first_sentence_prediction(model, tokenizer, [sentence])\n",
    "        print(f\"Chunk {idx+1} -  Sentence {sentences.index(sentence) + 1}: Sentence Label: {label_mapping[sentence_label]}, {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences\n",
      "['They drank the pub.']\n",
      "tokenized_texts\n",
      "[['[CLS]', 'they', 'drank', 'the', 'pub', '.', '[SEP]']]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm , trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Tokenize Inputs\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case = True)\n",
    "# Replace 'path_to_model_file.pth' with the path to the saved .pth model file.\n",
    "model_file = '/home/ubuntu/ritesh_manchikanti/cat_poc_model_2/bert-based-uncased-GED.pth'\n",
    "\n",
    "# Create an instance of the model's class\n",
    "# model = BertForSequenceClassification()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                      num_labels=2)\n",
    "# Load the saved state dictionary into the model\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# Optional: If you want to use the model for inference, move it to the appropriate device (GPU or CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# run on a sample text\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Create sentence) and label lists\n",
    "sentences = [\"They drank the pub.\"]\n",
    "# We need to add special tokens at the beginning and end of each sentence\n",
    "# for BERT to work properly\n",
    "print(\"sentences\")\n",
    "print(sentences)\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels =[0]\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(\"tokenized_texts\")\n",
    "print(tokenized_texts)\n",
    "# Padding Sentences\n",
    "# Set the maximum sequence length. The longest sequence in our training set\n",
    "# is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "predictions = []\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(\n",
    "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "    maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "    )\n",
    "# Index Numbers and Padding\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# pad sentences\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
    "# Attention masks\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i > 0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  # Forward pass, calculate logit predictions\n",
    "  logits = model(prediction_inputs.to(device), token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "# Move logits and labels to CPU\n",
    "logits = logits.detach().cpu().numpy()\n",
    "# label_ids = b_labels.to(\"cpu\").numpy()\n",
    "# Store predictions and true labels\n",
    "predictions.append(logits)\n",
    "# true_labels.append(label_ids)\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "# flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "print(flat_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
