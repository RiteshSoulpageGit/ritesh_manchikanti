{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=164, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=164,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "\n",
    "## Testing the model on text\n",
    "text = ''' I want to dedicate this book to my family. Rachel, Connor, and Ben, you are my life. To my \n",
    "wonderful mother, Susan, and my sister, Kristy, - thank you for the love and support. I also \n",
    "would like to thank the faculty, staff, and students at both Notre Dame of Maryland \n",
    "University and Johns Hopkins University for allowing me to fulfill my passion for teaching \n",
    "the teachers of today and tomorrow. -Ryan L. Schaaf \n",
    " \n",
    "I want to dedicate this book to my parents, who always promoted the value of a strong \n",
    "education. From installing my pretend classroom in our basement and pretending to be my \n",
    "first students, to encouraging me throughout college and graduate school, I am forever \n",
    "grateful. To my husband, Nick, thank you for being my first editor and always allowing me to \n",
    "bounce ideas off of you. We are a team. Thank you for always supporting me. Ian and Ryan, \n",
    "thank you for this unbelievable opportunity and all that you have taught me throughout this \n",
    "experience. -Becky Zayas \n",
    " \n",
    "This book is intended to celebrate the exceptional dedication and courage educators have \n",
    "exhibited, and to acknowledge their demonstrated capacity to adapt and innovate in \n",
    "extraordinarily challenging and uncertain conditions. Now is the time for us to recognize the \n",
    "exceptional role they play, and to empower them with the training, professional development, \n",
    "support, and working conditions needed to effectively deploy their talents. For the education \n",
    "system to recover from the COVID pandemic requires sustained investment in the well-\n",
    "being, training, professional development and working conditions of the worldâ€™s 71 million \n",
    "educators. Education recovery will only be successful if it is conducted hand-in-hand with \n",
    "teachers, giving them both voice and agency to participate in the critical change process. - Ian \n",
    "Jukes \n",
    "    \n",
    " '''\n",
    "\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def load_model():\n",
    "    model_name = \"bert-base-uncased\"  # Replace with the appropriate pre-trained model name\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    return model, tokenizer\n",
    "\n",
    "def classify_grammar(sentence, model, tokenizer):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class_index = torch.argmax(probabilities).item()\n",
    "    label_mapping = {0: \"Grammatically Correct\", 1: \"Grammatically Incorrect\"}\n",
    "    predicted_class_label = label_mapping[predicted_class_index]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Example usage\n",
    "sentence = \"She don't likes ice cream..\"\n",
    "\n",
    "# \"She don't likes ice cream.\"\n",
    "# \"He plays the piano and tennis.\"\n",
    "# \"My dogs eat cookies and eats cake.\"\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "predicted_label = classify_grammar(sentence, model, tokenizer)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm , trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import nltk\n",
    "# nltk.download('punkt')  # Download the punkt tokenizer for sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved state dictionary into the model\n",
    "model_file = '/home/ubuntu/ritesh_manchikanti/cat_poc_model_2/bert-based-uncased-GED.pth'\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        for page in PDFPage.get_pages(file, check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "            text += fake_file_handle.getvalue()\n",
    "        \n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to divide text into chunks of 512 tokens\n",
    "def divide_text_into_chunks(text, max_chunk_size=512):\n",
    "    tokenized_texts = text.split()  # Assuming the text is already tokenized by spaces, adjust as needed\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for tokenized_text in tokenized_texts:\n",
    "        if len(current_chunk) + len(tokenized_text) <= max_chunk_size:\n",
    "            current_chunk += tokenized_text + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = tokenized_text + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "text_splitter_1 = TokenTextSplitter(chunk_size=100,chunk_overlap = 2)\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './15031-4983-FullBook.pdf'\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = text_splitter_1.split_text(text)\n",
    "# chunks = divide_text_into_chunks(text)\n",
    "\n",
    "# Predict each chunk\n",
    "\n",
    "for idx, chunk in enumerate(chunks[:1]):\n",
    "    print(len(chunk.split()))\n",
    "    sentences = sent_tokenize(chunk)\n",
    "    print(sentences)\n",
    "    # print(idx,chunk)\n",
    "    # for sent in chunk:\n",
    "        # print(sent)\n",
    "    # tokenized_texts = [tokenizer.tokenize(sent) for sent in chunk]\n",
    "    \n",
    "# for idx, chunk in enumerate(chunks[:1]):\n",
    "#     sentences = sent_tokenize(chunk)\n",
    "#     # We need to add special tokens at the beginning and end of each sentence\n",
    "#     # for BERT to work properly\n",
    "#     sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "#     labels = [0]  # Assuming you have label information for each sentence, you can modify this accordingly\n",
    "\n",
    "#     tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    \n",
    "#     tokenized_texts = [tokenizer.tokenize(sent) for sent in chunk]\n",
    "#     MAX_LEN = 512\n",
    "#     input_ids = pad_sequences(\n",
    "#         [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "#         maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "#     )\n",
    "    \n",
    "#     # Convert to tensors\n",
    "#     prediction_inputs = torch.tensor(input_ids)\n",
    "#     prediction_masks = torch.tensor([[float(i > 0) for i in seq] for seq in input_ids])\n",
    "    \n",
    "#     # Put model in evaluation mode\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass, calculate logit predictions\n",
    "#         logits = model(prediction_inputs.to(device), token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "    \n",
    "#     # Move logits to CPU and store predictions for each sentence\n",
    "#     logits = logits.detach().cpu().numpy()\n",
    "#     flat_predictions = np.argmax(logits, axis=1).flatten()\n",
    "    \n",
    "#     # Print the predictions for each sentence in the chunk\n",
    "#     for i, predicted_class_index in enumerate(flat_predictions):\n",
    "#         label_mapping = {0: \"Grammatically Correct\", 1: \"Grammatically Incorrect\"}\n",
    "#         predicted_class_label = label_mapping[predicted_class_index]\n",
    "#         print(f\"Chunk {idx+1} - Sentence {i+1}: {predicted_class_label}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm , trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Tokenize Inputs\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case = True)\n",
    "# Replace 'path_to_model_file.pth' with the path to the saved .pth model file.\n",
    "model_file = '/home/ubuntu/ritesh_manchikanti/cat_poc_model_2/bert-based-uncased-GED.pth'\n",
    "\n",
    "# Create an instance of the model's class\n",
    "# model = BertForSequenceClassification()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                      num_labels=2)\n",
    "# Load the saved state dictionary into the model\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# Optional: If you want to use the model for inference, move it to the appropriate device (GPU or CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# run on a sample text\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Create sentence) and label lists\n",
    "sentences = [\"They drank the pub.\"]\n",
    "# We need to add special tokens at the beginning and end of each sentence\n",
    "# for BERT to work properly\n",
    "print(\"sentences\")\n",
    "print(sentences)\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels =[0]\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print(\"tokenized_texts\")\n",
    "print(tokenized_texts)\n",
    "# Padding Sentences\n",
    "# Set the maximum sequence length. The longest sequence in our training set\n",
    "# is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "predictions = []\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(\n",
    "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "    maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "    )\n",
    "# Index Numbers and Padding\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# pad sentences\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
    "# Attention masks\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i > 0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  # Forward pass, calculate logit predictions\n",
    "  logits = model(prediction_inputs.to(device), token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "# Move logits and labels to CPU\n",
    "logits = logits.detach().cpu().numpy()\n",
    "# label_ids = b_labels.to(\"cpu\").numpy()\n",
    "# Store predictions and true labels\n",
    "predictions.append(logits)\n",
    "# true_labels.append(label_ids)\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "# flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "print(flat_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
