{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import torch\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xxl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebb54647a8f48998b71f038cad6a457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342ca592775041b4bc69e4f00566fa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64a1eb6a0e043f4a17c82b68a84e8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d353e7b0a0aa4b0d9f466d8bfcd1b3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b064a7e319440efbd3cbb0e71ef6f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eee3021efb47628bf591c11b75dcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca9008d83f0497897ab9fed5c1cb9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-xl\")\n",
    "input_text = ' I love eating, apples are my favorite fruit.'\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_length=256,num_return_sequences = 2)\n",
    "edited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "edited_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love to eat, apples are my favorite fruit.', 'Today the weather is very nice.', 'She loves to sing, dance, and act!']\n"
     ]
    }
   ],
   "source": [
    "input_texts = ['I love eating, apples are my favorite fruit.', 'The weather is nice today.', 'She loves to sing, dance, and act!']\n",
    "# Tokenize and pad the sentences\n",
    "input_ids = [tokenizer(text, return_tensors=\"pt\").input_ids[0] for text in input_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=256, dtype=\"long\", truncating=\"post\", padding=\"post\", value=tokenizer.pad_token_id)\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "# Generate outputs\n",
    "outputs = model.generate(input_ids, max_length=256)\n",
    "# Decode the outputs\n",
    "edited_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "print(edited_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d3df1f6cd6471eb5ee403d6ba90fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/44.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrammarly/coedit-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m,model\u001b[38;5;241m.\u001b[39mget_memory_footprint())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# input_text = 'Fix grammatical errors in this sentence: When I grow up, I start to understand what he said is quite right.'\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(input_ids, max_length=256)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# edited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2543\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2540\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2541\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 2543\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2544\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[1;32m   2545\u001b[0m         )\n\u001b[1;32m   2546\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   2547\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2549\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2550\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   2551\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   2552\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/hub.py:418\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    416\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    419\u001b[0m         path_or_repo_id,\n\u001b[1;32m    420\u001b[0m         filename,\n\u001b[1;32m    421\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    422\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    423\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    424\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    425\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    426\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    427\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    428\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    429\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    430\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:544\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n\u001b[0;32m--> 544\u001b[0m         temp_file\u001b[39m.\u001b[39;49mwrite(chunk)\n\u001b[1;32m    546\u001b[0m \u001b[39mif\u001b[39;00m expected_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m expected_size \u001b[39m!=\u001b[39m temp_file\u001b[39m.\u001b[39mtell():\n\u001b[1;32m    547\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConsistency check failed: file should be of size \u001b[39m\u001b[39m{\u001b[39;00mexpected_size\u001b[39m}\u001b[39;00m\u001b[39m but has size\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtemp_file\u001b[39m.\u001b[39mtell()\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mWe are sorry for the inconvenience. Please retry download and\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m pass `force_download=True, resume_download=False` as argument.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf the issue persists, please let us\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m know by opening an issue on https://github.com/huggingface/huggingface_hub.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/tempfile.py:483\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39m@_functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenize\n",
    "# r.from_pretrained(\"grammarly/coedit-xxl\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-xxl\")\n",
    "\n",
    "# pip install transformers accelerate bitsandbytes\n",
    "# model_id = \"bigscience/bloom-1b7\"\n",
    "model_id = \"grammarly/coedit-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "print(\"---\",model.get_memory_footprint())\n",
    "\n",
    "# input_text = 'Fix grammatical errors in this sentence: When I grow up, I start to understand what he said is quite right.'\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "# outputs = model.generate(input_ids, max_length=256)\n",
    "# edited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "text = \"Him goes to the store to buy a new phone. Me and him are planning to visit the beach tomorrow.\"\n",
    "matches = tool.check(text)\n",
    "len(matches)\n",
    "tool.close() # Call `close()` to shut off the server when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors are -  2\n",
      "The position of errors is : 0\n",
      "Him\n",
      "The replacement for that is : ['I', 'He', 'She', 'They']\n",
      "The position of errors is : 43\n",
      "e\n",
      "The replacement for that is : ['I']\n"
     ]
    }
   ],
   "source": [
    "def print_text_till_next_space(text, index):\n",
    "    # Find the index of the next space after the given index\n",
    "    next_space_index = text.find(' ', index)\n",
    "\n",
    "    # If no space is found, print the substring from the given index till the end of the text\n",
    "    if next_space_index == -1:\n",
    "        print(text[index:])\n",
    "    else:\n",
    "        # Print the substring from the given index till the next space\n",
    "        print(text[index:next_space_index])\n",
    "print(\"number of errors are - \",len(matches))\n",
    "for i in matches:\n",
    "    print(\"The position of errors is :\",i.offsetInContext)\n",
    "    print(\"The error : \",text[i.offsetInContext])\n",
    "    # print_text_till_next_space(text,i.offsetInContext)\n",
    "    print(\"The replacement for that is :\",i.replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Match({'ruleId': 'WRONG_PRP_AT_SENT_START', 'message': 'The subject form of the pronoun may be required here.', 'replacements': ['I', 'He', 'She', 'They'], 'offsetInContext': 0, 'context': 'Him goes to the store to buy a new phone. M...', 'offset': 0, 'errorLength': 3, 'category': 'GRAMMAR', 'ruleIssueType': 'grammar', 'sentence': 'Him goes to the store to buy a new phone.'}), Match({'ruleId': 'CONFUSION_OF_ME_I', 'message': 'When referring to yourself and somebody else, put their name first. Also check whether the nominative form “I” is required here.', 'replacements': ['I'], 'offsetInContext': 43, 'context': '...m goes to the store to buy a new phone. Me and him are planning to visit the beach...', 'offset': 42, 'errorLength': 2, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Me and him are planning to visit the beach tomorrow.'})]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
