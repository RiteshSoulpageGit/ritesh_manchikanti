{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from hugchat import hugchat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "\n",
    "def chat_with_bot(text):\n",
    "    \n",
    "    chatbot = hugchat.ChatBot(cookie_path=\"/home/ubuntu/cookies.json\")\n",
    "    try:\n",
    "        response = chatbot.chat(text,temperature=0.2,\n",
    "                                top_k=95,\n",
    "                                max_new_tokens=512,\n",
    "                                )\n",
    "    except:\n",
    "        response = \"0\"\n",
    "    #print(text)\n",
    "    # Create a new conversation\n",
    "    \n",
    "    id = chatbot.new_conversation()\n",
    "    chatbot.change_conversation(id)\n",
    "    \n",
    "\n",
    "    # Get conversation list \n",
    "    conversation_list = chatbot.get_conversation_list()\n",
    "\n",
    "    return conversation_list, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(desired_files_df, max_sentence_length):\n",
    "    for idx in range(desired_files_df.shape[0]):\n",
    "        x = desired_files_df[\"text\"].iloc[idx]\n",
    "        # print(\"##\",x)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', x)\n",
    "        # print(\"**sentences:\",sentences)\n",
    "        print(len(sentences))\n",
    "        # break\n",
    "        # Adjust the maximum sentence length as per your requirements\n",
    "        chunks = sentences\n",
    "        final_chunks = []\n",
    "        new_data = \"\"\n",
    "        index = 0\n",
    "        index1 = -1\n",
    "        for c_index,sentence in enumerate(chunks[index:index1]):\n",
    "            # print(\"each sentence lenght:\", len(sentence.split(\" \")))\n",
    "            if len(sentence.split(\" \")) <= max_sentence_length and len((new_data+sentence).split(\" \"))<= max_sentence_length:\n",
    "                new_data = new_data+sentence\n",
    "            else:\n",
    "                # print(\"next setence lenght:\", index, len(sentence.split(\" \")))\n",
    "                final_chunks.append(new_data)\n",
    "                new_data = \"\"\n",
    "                index = c_index\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/cat_poc/llms/final_data.csv\"\n",
    "data_frame = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_frame[data_frame[\"filename\"] == \"15031-4983-FullBook.docx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 400  # Adjust the chunk size as per your requirements\n",
    "chunks_data = read_files(data, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter_1 = TokenTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "# text_splitter_2 = TokenTextSplitter(chunk_size=150, chunk_overlap=0)\n",
    "chunks = text_splitter_1.split_text(data['text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = chunks[5]\n",
    "# print(len(x.split(\" \")),x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = chunks_data[5]\n",
    "# print(len(x.split(\" \")),x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_beginners = [\n",
    "    # ''' Analyse  the following text  weather it has number_treatment give me respose in \"Yes\" or \"No\" only'''\n",
    "    # ''' count no of phonetics, symbols that are there in the doccument and give me the count number '''\n",
    "    '''count the number of phonetics and symbols present in the document and provide only the count just give me a number and return zero if there are none but return a number'''\n",
    "    # '''Analyse  the following text  weather it has Single_or_double_quotes and give me respose in \"Single\" or \"double\" only?:''',\n",
    "    # '''Analyse  the following text  weather it has Series_comma and give me respose in \"Yes\" or \"No\" only?:''',\n",
    "    # '''analyse  the below text  weather it is in \"American_english style\" or \"British english style and give me respose in \"American\" or \"British\" only?:\"'''\n",
    "]\n",
    "\n",
    "Modified_chunks = []\n",
    "for prompt in prompt_beginners:\n",
    "    prompt_line = f'''{prompt}'''\n",
    "    for chunk_i in chunks_data:\n",
    "        input_text = prompt_line + chunk_i\n",
    "        Modified_chunks.append(input_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Modified_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modified_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = \"\"\n",
    "count = 0\n",
    "\n",
    "for chunk_data in Modified_chunks[:]:\n",
    "    conversation_list, response = chat_with_bot(chunk_data)\n",
    "    # print(\"Conversation List:\", conversation_list)\n",
    "    # print(\"Response:\", response)\n",
    "    numbers = sum(int(num) for num in re.findall(r'\\d+', response))\n",
    "    count += numbers\n",
    "    response_text += response\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers = re.findall(r'\\d+', \"There are 45 phonetic characters and 1 symbol (the bullet point character) in this document\")\n",
    "total = sum(int(num) for num in re.findall(r'\\d+', \"There are 45 phonetic characters and 1 symbol (the bullet point character) in this document\"))\n",
    "print(\"Count of numbers:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Single_or_double_quotes', 'Series_comma','American_english',\"number_treatment\"])\n",
    "\n",
    "punctuational_error_count, grammatical_error_count, spelling_error_count,missing_articles_count =[],[],[],[]\n",
    "for chunk_data in Modified_chunks[:]:\n",
    "    conversation_list, response = chat_with_bot(chunk_data)\n",
    "    print(\"Conversation List:\", conversation_list)\n",
    "    print(\"Response:\", response)\n",
    "    print()\n",
    "\n",
    "    # Extract the numeric value using regular expressions\n",
    "    # error_count = re.findall(r'\\d+', response.split(\"\\n\")[0])\n",
    "\n",
    "    if error_count:\n",
    "        # Convert the extracted value to an integer\n",
    "        error_count = int(error_count[0])\n",
    "    else:\n",
    "        # If a numeric value is not found, try to convert words to numbers\n",
    "        try:\n",
    "            error_count = response.split(\"\\n\")[0]\n",
    "        except ValueError:\n",
    "            error_count = 0\n",
    "    \n",
    "    print(\"error_count:\", response)\n",
    "\n",
    "    # Determine the column to assign the error count based on the prompt\n",
    "    if \"Single_or_double_quotes\" in chunk_data:\n",
    "        # column_name = 'punctuational error_count'\n",
    "        punctuational_error_count.append(response)\n",
    "    if \"Series_comma\" in chunk_data:\n",
    "        # column_name = 'grammatical error_count'\n",
    "        grammatical_error_count.append(response)\n",
    "    if \"American_english \" in chunk_data:\n",
    "        # column_name = 'spelling_error_count'\n",
    "        spelling_error_count.append(response)\n",
    "    if \"number_treatment\" in chunk_data:\n",
    "        # column_name = 'missing_articles_count'\n",
    "        missing_articles_count.append(error_count)\n",
    "    else:\n",
    "        column_name = None\n",
    "\n",
    "    # print(\"column_name: \", column_name)\n",
    "\n",
    "    # # Add the error count to the dataframe if it is not None and column_name is valid\n",
    "    # if error_count is not None and column_name:\n",
    "    #     if column_name in df.columns:\n",
    "    #         df[column_name] = df.get(column_name, 0) + error_count\n",
    "    #     else:\n",
    "    #         df[column_name] = error_count\n",
    "\n",
    "print(\"punc\",punctuational_error_count,len(punctuational_error_count))\n",
    "print(\"Grammer\",grammatical_error_count,len(grammatical_error_count))\n",
    "print(\"spelling\",spelling_error_count,len(spelling_error_count))\n",
    "# print(\"arti\",missing_articles_count,len(missing_articles_count))\n",
    "df = pd.DataFrame({'punctuational error_count':punctuational_error_count,\n",
    "                   'grammatical error_count':grammatical_error_count, \n",
    "                   'spelling_error_count':spelling_error_count})\n",
    "                #    'missing_articles_count':missing_articles_count})\n",
    "# Print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df.rename(columns={\"punctuational error_count\": \"Single_or_double_quotes\", \"grammatical error_count\": \"Series_comma\",\"American_english\":\"Style\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
