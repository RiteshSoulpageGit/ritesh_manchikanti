{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2868980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "from docx import Document\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "from word2number import w2n\n",
    "from hugchat import hugchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a393556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentAnalyzer_content:\n",
    "    def __init__(self, file_path, max_sentence_length=400, no_of_selected_chunks=4):\n",
    "        self.file_path = file_path\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.no_of_selected_chunks = no_of_selected_chunks\n",
    "        \n",
    "\n",
    "    def analyze_document(self):\n",
    "        def read_docx(file_path,max_sentence_length):\n",
    "            # Implementation of read_docx() function\n",
    "            chunk_dict = {\"chunks\":[], \"filenames\":[]}\n",
    "            path_file = file_path.split(\"/\")[-1]\n",
    "            doc = Document(self.file_path)\n",
    "            paragraphs = [p.text for p in doc.paragraphs if len(p.text)>1]\n",
    "            # print(paragraphs[0:20])\n",
    "\n",
    "            # Join paragraphs into a single string\n",
    "            document = ' '.join(paragraphs)\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', document)\n",
    "            chunks = sentences\n",
    "\n",
    "            final_chunks = []\n",
    "            final_filenames = []\n",
    "            new_data = \"\"\n",
    "            index = 0\n",
    "            index1 = -1\n",
    "            for c_index,sentence in enumerate(chunks[index:index1]):\n",
    "\n",
    "                if len(sentence.split(\" \")) <= max_sentence_length and len((new_data+sentence).split(\" \"))<= max_sentence_length:\n",
    "                    new_data = new_data+sentence\n",
    "                else:\n",
    "                    final_chunks.append(new_data)\n",
    "                    new_data = \"\"\n",
    "                    index = c_index\n",
    "\n",
    "            chunk_dict[\"chunks\"].append(final_chunks)\n",
    "            chunk_dict[\"filenames\"].append(path_file)\n",
    "\n",
    "\n",
    "            return chunk_dict\n",
    "\n",
    "\n",
    "        def chat_with_bot(text):\n",
    "            # Implementation of chat_with_bot() function\n",
    "            chatbot = hugchat.ChatBot(cookie_path=\"/home/ubuntu/cookies.json\")\n",
    "            try:\n",
    "                response = chatbot.chat(text,temperature=0.2,\n",
    "                                        top_k=95,\n",
    "                                        max_new_tokens=512,\n",
    "                                        )\n",
    "            except:\n",
    "                response = \"NO\"\n",
    "            #print(text)\n",
    "            # Create a new conversation\n",
    "\n",
    "            id = chatbot.new_conversation()\n",
    "            chatbot.change_conversation(id)\n",
    "\n",
    "\n",
    "            # Get conversation list\n",
    "            conversation_list = chatbot.get_conversation_list()\n",
    "\n",
    "            return conversation_list, response\n",
    "\n",
    "        def process_dataframe(df):\n",
    "            print(\"self.file_path\",self.file_path)\n",
    "            print(\"*\"*50)\n",
    "            \n",
    "            total_count = df['phonetic_symbol_count'].tolist()\n",
    "            \n",
    "            total_count = sum([int(i) for i in total_count])\n",
    "            print(\"total_count\",total_count)\n",
    "            \n",
    "            \n",
    "            missing_transposition = df['missing_transpositions'].tolist()\n",
    "            missing_transposition_lowercase_values = [value.lower() for value in missing_transposition]\n",
    "            print(\"missing_transposition_lowercase_values\",missing_transposition_lowercase_values)\n",
    "\n",
    "            quotes = df['Single_or_double_quotes'].tolist()\n",
    "            quotes_lower_case = [value.lower() for value in quotes]\n",
    "\n",
    "\n",
    "            Series_comma = df['Series_comma'].tolist()\n",
    "            Series_comma_lowercase_values = [value.lower() for value in Series_comma]\n",
    "\n",
    "\n",
    "            American_english = df['American_english'].tolist()\n",
    "            American_english_lowercase_values = [value.lower() for value in American_english]\n",
    "\n",
    "\n",
    "            rewrit_document = df['rewrit_document'].tolist()\n",
    "            rewrit_document_lowercase_values = [value.lower() for value in rewrit_document]\n",
    "\n",
    "\n",
    "            subject_matter_expertise = df['subject_matter_expertise'].tolist()\n",
    "            subject_matter_expertise_lowercase_values = [value.lower() for value in subject_matter_expertise]\n",
    "\n",
    "            mutiple_writting_styles = df['mutiple_writting_styles'].tolist()\n",
    "            mutiple_writting_styles_lowercase_values = [value.lower() for value in mutiple_writting_styles]\n",
    "\n",
    "            merged_dict = {}\n",
    "            name_file = self.file_path.split('/')[-1]\n",
    "            merged_dict.update({'file_path': name_file})\n",
    "            print(\"#\"*50)\n",
    "            # Check missing_transpositions column\n",
    "            \n",
    "            merged_dict.update({'phonetic_symbol_count': [total_count]})\n",
    "                \n",
    "            if 'yes.' in missing_transposition_lowercase_values:\n",
    "                print(\"#\"*50)\n",
    "                merged_dict.update({'missing_transpositions': ['inconsistent']})\n",
    "            else:\n",
    "                merged_dict.update({'missing_transpositions': ['consistent']})\n",
    "\n",
    "            if 'double' in quotes_lower_case and 'single' in quotes_lower_case:\n",
    "                merged_dict.update({'Single_or_double_quotes': ['inconsistent']})\n",
    "            else:\n",
    "                merged_dict.update({'Single_or_double_quotes': ['consistent']})\n",
    "\n",
    "            if 'no' in  Series_comma_lowercase_values:\n",
    "                merged_dict.update({'Series_comma': ['inconsistent']})\n",
    "            else:\n",
    "                merged_dict.update({'Series_comma': ['consistent']})\n",
    "\n",
    "            if 'british' in American_english_lowercase_values and American_english_lowercase_values:\n",
    "                merged_dict.update({'American_english': ['inconsistent has combination of british and american style']})\n",
    "            else:\n",
    "                merged_dict.update({'American_english': ['consistent']})\n",
    "\n",
    "            if 'no' in rewrit_document_lowercase_values:\n",
    "                merged_dict.update({'rewrit_document': ['consistent']})\n",
    "            else:\n",
    "                merged_dict.update({'rewrit_document': ['inconsistent']})\n",
    "\n",
    "            if 'yes' in subject_matter_expertise_lowercase_values:\n",
    "                merged_dict.update({'subject_matter_expertise': ['require subject matter expertise']})\n",
    "            else:\n",
    "                merged_dict.update({'subject_matter_expertise': ['no-require subject matter expertise']})\n",
    "\n",
    "            if 'yes' in mutiple_writting_styles_lowercase_values:\n",
    "                merged_dict.update({'mutiple_writting_styles': ['Has multiple writting styles']})\n",
    "            else:\n",
    "                merged_dict.update({'mutiple_writting_styles': ['NO multiple writting styles']})\n",
    "\n",
    "            merged_df = pd.DataFrame.from_dict(merged_dict)\n",
    "\n",
    "            return merged_df\n",
    "\n",
    "\n",
    "        def extract_error_count(response):\n",
    "            # Implementation of extract_error_count() function\n",
    "            error_count = re.findall(r'\\d+', response.split(\"\\n\")[0])\n",
    "\n",
    "            if error_count:\n",
    "                # Convert the extracted value to an integer\n",
    "                error_count = int(error_count[0])\n",
    "            else :\n",
    "                # If a numeric value is not found, try to convert words to numbers\n",
    "                try:\n",
    "                    error_count = response.split(\"\\n\")[0]\n",
    "                except ValueError:\n",
    "                    error_count = 0\n",
    "                    \n",
    "            return error_count\n",
    "\n",
    "        prompt_beginners = [\n",
    "\n",
    "    #    '''Are there any missing_transpositions in the document? give me response only in  \"YES\" or \"NO\":''',\n",
    "    #     '''Analyse  the following text  whether it has Single_or_double_quotes and give me respose in \"Single\" or \"double\" only?:''',\n",
    "    #     '''Analyse  the following text  whether it has Series_comma and give me respose in \"Yes\" or \"No\" only?:''',\n",
    "    #     '''Analyse  the following text  whether it is in \"American_english style\" or \"British english style and give me respose in \"American\" or \"British\" only?:\"''',\n",
    "    #     '''Analyse the following text do we need to rewrit_document ? give me response only in  \"YES\" or \"NO\":''',\n",
    "        '''count the number of phonetics and symbols present in the document and provide only the count just give me a number'''\n",
    "        # ''' count no of phonetics , symbols that are there in the doccument and give me the count number '''\n",
    "\n",
    "        \n",
    "        ]\n",
    "\n",
    "        data_dict = read_docx(self.file_path, self.max_sentence_length)\n",
    "\n",
    "        missing_transpositions = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        Single_or_double_quotes = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        Series_comma = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        American_english= [\"NULL\"]*self.no_of_selected_chunks\n",
    "        rewrit_document = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        subject_matter_expertise = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        mutiple_writting_styles = [\"NULL\"]*self.no_of_selected_chunks\n",
    "        phonetic_symbol_count = [0]*self.no_of_selected_chunks\n",
    "        modified_chunks = []\n",
    "        for prompt in prompt_beginners:\n",
    "            for chunk in data_dict[\"chunks\"][0][:self.no_of_selected_chunks]:\n",
    "                input_text = f\"{prompt} {chunk}\"\n",
    "                modified_chunks.append(input_text)\n",
    "            \n",
    "#         print(modified_chunks)\n",
    "\n",
    "        df_data = {\n",
    "                    'phonetic_symbol_count': phonetic_symbol_count,\n",
    "                   'missing_transpositions':missing_transpositions,\n",
    "                   'Single_or_double_quotes':Single_or_double_quotes, \n",
    "                   'Series_comma':Series_comma, \n",
    "                   'American_english':American_english,\n",
    "                   'rewrit_document':rewrit_document,\n",
    "                   'subject_matter_expertise':subject_matter_expertise,\n",
    "                   'mutiple_writting_styles':mutiple_writting_styles\n",
    "        }\n",
    "        \n",
    "        for index, f_chunks in enumerate(modified_chunks[0:4]):\n",
    "            # print(f_chunks)\n",
    "            conversation_list, response = chat_with_bot(f_chunks)\n",
    "            # numbers = re.findall(r'\\d+', response)\n",
    "            # total = sum(int(num) for num in numbers)\n",
    "            # total_count += total\n",
    "            error_count = extract_error_count(response)\n",
    "            print(\"response\",response)\n",
    "            print(\"error_count\",error_count)\n",
    "            \n",
    "            if \"phonetics\" in  f_chunks:\n",
    "                # column_name = 'phonetic_symbol_count'\n",
    "                # total_count[index % self.no_of_selected_chunks] = error_count\n",
    "                # error_count = 23\n",
    "                phonetic_symbol_count[index % self.no_of_selected_chunks] = error_count if isinstance(error_count, int) else 0\n",
    "                # print(phonetic_symbol_count)\n",
    "\n",
    "            if \"missing_transpositions\" in f_chunks:\n",
    "                # column_name = 'punctuational error_count'\n",
    "                missing_transpositions[index % self.no_of_selected_chunks]  = error_count\n",
    "            if \"Single_or_double_quotes\" in f_chunks:\n",
    "                # column_name = 'grammatical error_count'\n",
    "                Single_or_double_quotes[index % self.no_of_selected_chunks] = error_count\n",
    "            if \"Series_comma \" in f_chunks:\n",
    "                # column_name = 'spelling_error_count'\n",
    "                Series_comma[index % self.no_of_selected_chunks] = error_count\n",
    "            if \"American_english\" in f_chunks:\n",
    "                # column_name = 'missing_articles_count'\n",
    "                American_english[index % self.no_of_selected_chunks] = error_count\n",
    "            if \"rewrit_document\" in f_chunks:\n",
    "            # column_name = 'missing_articles_count'\n",
    "                rewrit_document[index % self.no_of_selected_chunks] = error_count\n",
    "            if \"subject_matter_expertise\" in f_chunks:\n",
    "            # column_name = 'missing_articles_count'\n",
    "                subject_matter_expertise[index % self.no_of_selected_chunks] = error_count\n",
    "            if \"mutiple_writting_styles\" in f_chunks:\n",
    "            # column_name = 'missing_articles_count'\n",
    "                mutiple_writting_styles[index % self.no_of_selected_chunks] = error_count\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "        processed_df = process_dataframe(df)\n",
    "        \n",
    "#         df.loc[\"Total\"] = df.sum()\n",
    "#         if (df > 1).any().any():\n",
    "#             print(\"Inconsistent\")\n",
    "#         else:\n",
    "#             print(\"Consistent\")\n",
    "        return processed_df\n",
    "\n",
    "# # Usage example:\n",
    "# analyzer = DocumentAnalyzer(file_path=\"/home/roufa/caption_detection/15031-4985-FullBook.docx\")\n",
    "# result_df = analyzer.analyze_document()\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc33f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response There are approximately 376 phonetic symbols and punctuation marks in the document you provided. However, please note that some of them may have been used multiple times, so the actual number of distinct symbols would likely be lower than this figure.\n",
      "error_count 376\n",
      "response NO\n",
      "error_count NO\n",
      "response NO\n",
      "error_count NO\n",
      "response NO\n",
      "error_count NO\n",
      "self.file_path /home/ubuntu/cat_poc/data/Aarons-Renamed-r01/15032-5196-FullBook.docx\n",
      "**************************************************\n",
      "total_count 376\n",
      "missing_transposition_lowercase_values ['null', 'null', 'null', 'null']\n",
      "##################################################\n",
      "                  file_path  phonetic_symbol_count missing_transpositions  \\\n",
      "0  15032-5196-FullBook.docx                    376             consistent   \n",
      "\n",
      "  Single_or_double_quotes Series_comma American_english rewrit_document  \\\n",
      "0              consistent   consistent       consistent    inconsistent   \n",
      "\n",
      "              subject_matter_expertise      mutiple_writting_styles  \n",
      "0  no-require subject matter expertise  NO multiple writting styles  \n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "analyzer = DocumentAnalyzer_content(file_path=\"/home/ubuntu/cat_poc/data/Aarons-Renamed-r01/15032-5196-FullBook.docx\")\n",
    "result_df = analyzer.analyze_document()\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427557d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>phonetic_symbol_count</th>\n",
       "      <th>missing_transpositions</th>\n",
       "      <th>Single_or_double_quotes</th>\n",
       "      <th>Series_comma</th>\n",
       "      <th>American_english</th>\n",
       "      <th>rewrit_document</th>\n",
       "      <th>subject_matter_expertise</th>\n",
       "      <th>mutiple_writting_styles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15032-5196-FullBook.docx</td>\n",
       "      <td>376</td>\n",
       "      <td>consistent</td>\n",
       "      <td>consistent</td>\n",
       "      <td>consistent</td>\n",
       "      <td>consistent</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>no-require subject matter expertise</td>\n",
       "      <td>NO multiple writting styles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file_path  phonetic_symbol_count missing_transpositions  \\\n",
       "0  15032-5196-FullBook.docx                    376             consistent   \n",
       "\n",
       "  Single_or_double_quotes Series_comma American_english rewrit_document  \\\n",
       "0              consistent   consistent       consistent    inconsistent   \n",
       "\n",
       "              subject_matter_expertise      mutiple_writting_styles  \n",
       "0  no-require subject matter expertise  NO multiple writting styles  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1107cec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/roufa/Apex_document'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/roufa/Apex_document/result_prompt.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3721\u001b[0m     path_or_buf,\n\u001b[1;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3737\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/io/common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 734\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    737\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    738\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/pandas/io/common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/roufa/Apex_document'"
     ]
    }
   ],
   "source": [
    "result_df.to_csv(\"/home/roufa/Apex_document/result_prompt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc181634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810f08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787c4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_transposition = df['missing_transpositions'].tolist()\n",
    "missing_transposition_lowercase_values = [value.lower() for value in missing_transposition]\n",
    "\n",
    "quotes = df['Single_or_double_quotes'].tolist()\n",
    "quotes_lower_case = [value.lower() for value in quotes]\n",
    "\n",
    "\n",
    "\n",
    "Series_comma = df['Series_comma'].tolist()\n",
    "Series_comma_lowercase_values = [value.lower() for value in Series_comma]\n",
    "\n",
    "\n",
    "American_english = df['American_english'].tolist()\n",
    "American_english_lowercase_values = [value.lower() for value in American_english]\n",
    "\n",
    "\n",
    "rewrit_document = df['rewrit_document'].tolist()\n",
    "rewrit_document_lowercase_values = [value.lower() for value in rewrit_document]\n",
    "\n",
    "\n",
    "subject_matter_expertise = df['subject_matter_expertise'].tolist()\n",
    "subject_matter_expertise_lowercase_values = [value.lower() for value in subject_matter_expertise]\n",
    "\n",
    "mutiple_writting_styles = df['mutiple_writting_styles'].tolist()\n",
    "mutiple_writting_styles_lowercase_values = [value.lower() for value in mutiple_writting_styles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_dataframe(df):\n",
    "    print(\"*\"*50)\n",
    "    missing_transposition = df['missing_transpositions'].tolist()\n",
    "    missing_transposition_lowercase_values = [value.lower() for value in missing_transposition]\n",
    "    print(\"missing_transposition_lowercase_values\",missing_transposition_lowercase_values)\n",
    "\n",
    "    quotes = df['Single_or_double_quotes'].tolist()\n",
    "    quotes_lower_case = [value.lower() for value in quotes]\n",
    "\n",
    "\n",
    "\n",
    "    Series_comma = df['Series_comma'].tolist()\n",
    "    Series_comma_lowercase_values = [value.lower() for value in Series_comma]\n",
    "\n",
    "\n",
    "    American_english = df['American_english'].tolist()\n",
    "    American_english_lowercase_values = [value.lower() for value in American_english]\n",
    "\n",
    "\n",
    "    rewrit_document = df['rewrit_document'].tolist()\n",
    "    rewrit_document_lowercase_values = [value.lower() for value in rewrit_document]\n",
    "\n",
    "\n",
    "    subject_matter_expertise = df['subject_matter_expertise'].tolist()\n",
    "    subject_matter_expertise_lowercase_values = [value.lower() for value in subject_matter_expertise]\n",
    "\n",
    "    mutiple_writting_styles = df['mutiple_writting_styles'].tolist()\n",
    "    mutiple_writting_styles_lowercase_values = [value.lower() for value in mutiple_writting_styles]\n",
    "\n",
    "    merged_dict = {}\n",
    "    print(\"#\"*50)\n",
    "    # Check missing_transpositions column\n",
    "    if 'yes.' in missing_transposition_lowercase_values:\n",
    "        print(\"#\"*50)\n",
    "        merged_dict.update({'missing_transpositions': ['inconsistent']})\n",
    "    else:\n",
    "        merged_dict.update({'missing_transpositions': ['consistent']})\n",
    "    \n",
    "    if 'double' in quotes_lower_case and 'single' in quotes_lower_case:\n",
    "        merged_dict.update({'Single_or_double_quotes': ['inconsistent']})\n",
    "    else:\n",
    "        merged_dict.update({'Single_or_double_quotes': ['consistent']})\n",
    "    \n",
    "    if 'no' in  Series_comma_lowercase_values:\n",
    "        merged_dict.update({'Series_comma': ['inconsistent']})\n",
    "    else:\n",
    "        merged_dict.update({'Series_comma': ['consistent']})\n",
    "    \n",
    "    if 'british' in American_english_lowercase_values and American_english_lowercase_values:\n",
    "        merged_dict.update({'American_english': ['inconsistent has combination of british and american style']})\n",
    "    else:\n",
    "        merged_dict.update({'American_english': ['consistent']})\n",
    "    \n",
    "    if 'no' in rewrit_document_lowercase_values:\n",
    "        merged_dict.update({'rewrit_document': ['consistent']})\n",
    "    else:\n",
    "        merged_dict.update({'rewrit_document': ['inconsistent']})\n",
    "    \n",
    "    if 'yes' in subject_matter_expertise_lowercase_values:\n",
    "        merged_dict.update({'subject_matter_expertise': ['require subject matter expertise']})\n",
    "    else:\n",
    "        merged_dict.update({'subject_matter_expertise': ['no-require subject matter expertise']})\n",
    "    \n",
    "    if 'yes' in mutiple_writting_styles_lowercase_values:\n",
    "        merged_dict.update({'mutiple_writting_styles': ['Has multiple writting styles']})\n",
    "    else:\n",
    "        merged_dict.update({'mutiple_writting_styles': ['NO multiple writting styles']})\n",
    "    \n",
    "    merged_df = pd.DataFrame.from_dict(merged_dict)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bba35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_dataframe(result_df)\n",
    "print(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25097406",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00929ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
