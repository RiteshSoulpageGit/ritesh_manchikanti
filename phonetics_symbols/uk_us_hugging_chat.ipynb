{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from hugchat import hugchat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "\n",
    "def chat_with_bot(text):\n",
    "    \n",
    "    chatbot = hugchat.ChatBot(cookie_path=\"/home/ubuntu/cookies.json\")\n",
    "    try:\n",
    "        response = chatbot.chat(text,temperature=0.2,\n",
    "                                top_k=95,\n",
    "                                max_new_tokens=512,\n",
    "                                )\n",
    "    except:\n",
    "        response = \"0\"\n",
    "    #print(text)\n",
    "    # Create a new conversation\n",
    "    \n",
    "    id = chatbot.new_conversation()\n",
    "    chatbot.change_conversation(id)\n",
    "    \n",
    "\n",
    "    # Get conversation list \n",
    "    conversation_list = chatbot.get_conversation_list()\n",
    "\n",
    "    return conversation_list, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(desired_files_df, max_sentence_length):\n",
    "    for idx in range(desired_files_df.shape[0]):\n",
    "        x = desired_files_df[\"text\"].iloc[idx]\n",
    "        # print(\"##\",x)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', x)\n",
    "        # print(\"**sentences:\",sentences)\n",
    "        print(len(sentences))\n",
    "        # break\n",
    "        # Adjust the maximum sentence length as per your requirements\n",
    "\n",
    "        chunks = sentences\n",
    "\n",
    "        final_chunks = []\n",
    "        new_data = \"\"\n",
    "        index = 0\n",
    "        index1 = -1\n",
    "\n",
    "        for c_index,sentence in enumerate(chunks[index:index1]):\n",
    "            # print(\"each sentence lenght:\", len(sentence.split(\" \")))\n",
    "            if len(sentence.split(\" \")) <= max_sentence_length and len((new_data+sentence).split(\" \"))<= max_sentence_length:\n",
    "                new_data = new_data+sentence\n",
    "            else:\n",
    "                # print(\"next setence lenght:\", index, len(sentence.split(\" \")))\n",
    "                final_chunks.append(new_data)\n",
    "                new_data = \"\"\n",
    "                index = c_index\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/cat_poc/llms/final_data.csv\"\n",
    "data_frame = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15031-4983-FullBook.docx</td>\n",
       "      <td>Learner Choice, Learning Voice\\n\\n\\nLearner Vo...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15031-4984-FullBook.docx</td>\n",
       "      <td>\\n\\nExistentialism: A Philosophical Inquiry\\n\\...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15031-4985-FullBook.docx</td>\n",
       "      <td>\"The editors of this volume are the top practi...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15031-4986-FullBook.docx</td>\n",
       "      <td>Black Power Music!\\n\\nBlack Power Music!: Prot...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15031-4989-FullBook.docx</td>\n",
       "      <td>Home\\n\\n\\nHome articulates a ‘critical geograp...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>15032-5774-FullBook.docx</td>\n",
       "      <td>The Japanese LGBTQ+ Community in the World\\n\\n...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>15032-5789-FullBook.docx</td>\n",
       "      <td>The Acquisition of English Grammar and Phonolo...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>15032-5829-FullBook.docx</td>\n",
       "      <td>\\n‘New concepts, new words for them, new actio...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>15032-5843-FullBook.docx</td>\n",
       "      <td>Translating Rumi into the West\\n\\n\\nFocusing o...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>15032-6039-FullBook.docx</td>\n",
       "      <td>Reassessing Vocational Education in China\\n\\nB...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0     15031-4983-FullBook.docx   \n",
       "1     15031-4984-FullBook.docx   \n",
       "2     15031-4985-FullBook.docx   \n",
       "3     15031-4986-FullBook.docx   \n",
       "4     15031-4989-FullBook.docx   \n",
       "...                        ...   \n",
       "1231  15032-5774-FullBook.docx   \n",
       "1232  15032-5789-FullBook.docx   \n",
       "1233  15032-5829-FullBook.docx   \n",
       "1234  15032-5843-FullBook.docx   \n",
       "1235  15032-6039-FullBook.docx   \n",
       "\n",
       "                                                   text    level  \n",
       "0     Learner Choice, Learning Voice\\n\\n\\nLearner Vo...  Level 1  \n",
       "1     \\n\\nExistentialism: A Philosophical Inquiry\\n\\...  Level 1  \n",
       "2     \"The editors of this volume are the top practi...  Level 1  \n",
       "3     Black Power Music!\\n\\nBlack Power Music!: Prot...  Level 1  \n",
       "4     Home\\n\\n\\nHome articulates a ‘critical geograp...  Level 1  \n",
       "...                                                 ...      ...  \n",
       "1231  The Japanese LGBTQ+ Community in the World\\n\\n...  Level 3  \n",
       "1232  The Acquisition of English Grammar and Phonolo...  Level 3  \n",
       "1233  \\n‘New concepts, new words for them, new actio...  Level 3  \n",
       "1234  Translating Rumi into the West\\n\\n\\nFocusing o...  Level 3  \n",
       "1235  Reassessing Vocational Education in China\\n\\nB...  Level 3  \n",
       "\n",
       "[1236 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_frame[data_frame[\"filename\"] == \"15031-4983-FullBook.docx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3256\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 400  # Adjust the chunk size as per your requirements\n",
    "\n",
    "chunks_data = read_files(data, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IPAString' from 'ipapy' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipapy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipapy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IPAString\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_ipa_symbols\u001b[39m(text):\n\u001b[1;32m      4\u001b[0m     ipa_symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IPAString' from 'ipapy' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipapy/__init__.py)"
     ]
    }
   ],
   "source": [
    "from ipapy import IPAString\n",
    "\n",
    "def find_ipa_symbols(text):\n",
    "    ipa_symbols = set()\n",
    "    ipa_string = IPAString(unicode_string=text, ignore=True)\n",
    "    for ipa_char in ipa_string:\n",
    "        ipa_symbols.add(ipa_char.symbol)\n",
    "    return ipa_symbols\n",
    "\n",
    "\n",
    "text = \"This is a sample [ˈtɛkst] with IPA symbols [ˈaɪˈpiːˈeɪ] and [sɪmˈbəʊlz].\"\n",
    "ipa_symbols = find_ipa_symbols(text)\n",
    "for symbol in ipa_symbols:\n",
    "    print(symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_valid' from 'ipapy.ipastring' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipapy/ipastring.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from ipapy import ipa_string\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # from ipapy import IPAString\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from ipapy import is_valid_ipa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from ipapy import is_valid\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipastring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_valid\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_ipa_strings\u001b[39m(text):\n\u001b[1;32m      9\u001b[0m     ipa_strings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_valid' from 'ipapy.ipastring' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/ipapy/ipastring.py)"
     ]
    }
   ],
   "source": [
    "# from ipapy import ipa_string\n",
    "# # from ipapy import IPAString\n",
    "# from ipapy import is_valid_ipa\n",
    "# from ipapy import is_valid\n",
    "from ipapy.ipastring import is_valid\n",
    "\n",
    "\n",
    "def find_ipa_strings(text):\n",
    "    ipa_strings = []\n",
    "    current_ipa_string = \"\"\n",
    "    for char in text:\n",
    "        current_ipa_string += char\n",
    "        if is_valid(current_ipa_string):\n",
    "            ipa_strings.append(current_ipa_string)\n",
    "        elif not is_valid(char):\n",
    "            current_ipa_string = \"\"\n",
    "    return ipa_strings\n",
    "\n",
    "text = \"This is a sample [ˈtɛkst] with IPA symbols [ˈaɪˈpiːˈeɪ] and [sɪmˈbəʊlz].\"\n",
    "ipa_strings = find_ipa_strings(text)\n",
    "for ipa_string in ipa_strings:\n",
    "    print(ipa_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'IPAString' has no attribute 'is_valid_ipa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miˈtaljans ˈtail\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# check if s contains only IPA characters (yes)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mIPAString\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid_ipa\u001b[49m(s):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms is a valid IPA string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# create IPA string from Unicode string\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'IPAString' has no attribute 'is_valid_ipa'"
     ]
    }
   ],
   "source": [
    "from ipapy.ipastring import IPAString\n",
    "from ipapy.ipachar import IPADiacritic\n",
    "\n",
    "# Unicode string for \"Italian style\"\n",
    "s = u\"iˈtaljans ˈtail\"\n",
    "\n",
    "if IPAString.is_valid_ipa(s):\n",
    "    print(\"s is a valid IPA string\")\n",
    "\n",
    "s_ipa = IPAString(unicode_string=s)\n",
    "\n",
    "print(unicode(s_ipa))   # Python 2\n",
    "print(s_ipa)            # Python 3\n",
    "\n",
    "print(s_ipa)\n",
    "\n",
    "s_cv = s.cns_vwl                # vowels and consonants\n",
    "s_cvs = s.cns_vwl_str           # + stress marks\n",
    "s_cvsl = s.cns_vwl_str_len      # + lenght marks\n",
    "s_cvslw = s.cns_vwl_str_len_wb  # + word breaks\n",
    "\n",
    "\n",
    "for c_ipa in s_ipa:\n",
    "    print(c_ipa)\n",
    "    if isinstance(c_ipa, IPADiacritic):\n",
    "        print(c_ipa.properties)\n",
    "\n",
    "for c_ipa in [c for c in s_ipa if c.is_vowel]:\n",
    "    print(u\"%s => %s\" % (c_ipa, c_ipa.name))\n",
    "for c_ipa in [c for c in s_ipa if c.is_consonant]:\n",
    "    print(u\"%s => %s\" % (c_ipa, c_ipa.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IPA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create an IPA object from a string\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ipa_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhɛlˈoʊ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipa'"
     ]
    }
   ],
   "source": [
    "from ipa import IPA\n",
    "\n",
    "# Create an IPA object from a string\n",
    "ipa_string = \"hɛlˈoʊ\"\n",
    "ipa = IPA(ipa_string)\n",
    "\n",
    "# Access various properties of the IPA object\n",
    "print(\"IPA String:\", ipa.ipa_string)\n",
    "print(\"Segments:\", ipa.segments)\n",
    "print(\"Stress Index:\", ipa.stress_index)\n",
    "print(\"Vowels:\", ipa.vowels)\n",
    "print(\"Consonants:\", ipa.consonants)\n",
    "\n",
    "# Iterate over each segment and print its properties\n",
    "for segment in ipa.segments:\n",
    "    print(\"Segment:\", segment.symbol)\n",
    "    print(\"Is Vowel:\", segment.is_vowel)\n",
    "    print(\"Is Consonant:\", segment.is_consonant)\n",
    "\n",
    "# Convert the IPA object back to a string\n",
    "converted_string = str(ipa)\n",
    "print(\"Converted String:\", converted_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', ' ', 'ˈ', 'k', 'æ', 't', ' ', 'i', 's', ' ', 'ð', 'ə', 'r', '.']\n"
     ]
    }
   ],
   "source": [
    "from ipapy import is_valid_ipa\n",
    "\n",
    "def detect_ipa_symbols(text):\n",
    "    ipa_symbols = []\n",
    "    for char in text:\n",
    "        if is_valid_ipa(char):\n",
    "            ipa_symbols.append(char)\n",
    "    return ipa_symbols\n",
    "\n",
    "# Example usage\n",
    "text = \"The ˈkæt is ðər.\"\n",
    "ipa_symbols = detect_ipa_symbols(text)\n",
    "print(ipa_symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 ['But', 'beyond', 'that,', 'the', 'book', 'acts', 'as', 'a', 'catalyst', 'for', 'inspiration', 'that', 'helps', 'educators', 'reimagine', 'what', 'education', 'can', 'be', 'in', 'a', 'time', 'of', 'global', 'transformation', '-', 'because', 'not', 'only', 'has', 'the', 'world', 'changed', 'but', 'so', 'has', 'education.Enjoy!!!Dr.Nicky', 'Mohan\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nThe', 'Need', 'for', 'Teacher', 'and', 'Learner', 'Agency', 'In', 'Pandemic', 'Times\\n\\nThe', 'global', 'pandemic', 'has', 'had', 'a', 'profound', 'effect', 'on', 'traditional', 'learning', 'processes.Remote', 'learning', 'has', 'created', 'opportunities', 'for', 'learners', 'to', 'turn', 'down', 'the', 'volume', 'on', 'their', 'teachers', 'or', 'even', 'turn', 'them', 'off', 'completely.This', 'ability', 'to', 'tune', 'out', 'is', 'transforming', 'many', 'of', 'our', 'traditional', 'assumptions', 'about', 'teaching', 'and', 'learning.Even', 'if', 'conventional', 'compliance-focused', 'approaches', 'may', 'have', 'previously', 'worked', 'in', 'engaging', 'learners,', 'these', 'same', 'techniques', 'increasingly', 'won’t', 'be', 'as', 'effective', '--', 'particularly', 'in', 'a', 'time', 'where', 'we', 'will', 'almost', 'certainly', 'continue', 'to', 'experience', 'accelerated', 'rates', 'of', 'disruptive', 'change', 'in', 'our', 'world', '-', 'change', 'caused,', 'in', 'large', 'part,', 'by', 'the', 'pandemic.The', 'short', 'and', 'long-term', 'implications', 'of', 'this', 'emerging', 'reality', 'are', 'substantial.They', 'require', 'educators', 'to', 'reassess', 'our', 'relationship', 'with', 'learners', 'carefully', 'and', 'fundamentally', 'rethink', 'how', 'we', 'approach', 'teaching,', 'learning,', 'and', 'assessment', 'in', 'the', 'modern', 'world.Amid', 'pandemic', 'times,', 'educators', 'and', 'educational', 'leaders', 'struggle', 'with', 'a', 'wide', 'range', 'of', 'technical', 'and', 'adaptive', 'challenges.While', 'many', 'of', 'these', 'problems', 'are', 'technical,', 'overwhelmingly,', 'the', 'most', 'significant', 'challenges', 'are', 'adaptive.Moreover,', 'adaptive', 'challenges', 'reflect', 'issues', 'for', 'which', 'there', 'are', 'no', 'quickly', 'discernible', 'answers.Thus,', 'adaptive', 'challenges', 'cannot', 'be', 'instantly', 'solved', 'by', 'external', 'experts.Nor', 'can', 'solutions', 'be', 'implemented', 'by', 'mandate.Instead,', 'solving', 'adaptive', 'challenges', 'requires', 'experimentation', 'by', 'those', 'directly', 'experiencing', 'the', 'problem.In', 'earlier', 'times,', 'most', 'of', 'the', 'challenges', 'educators', 'faced', 'were', 'technical,', 'and', 'the', 'nature', 'of', 'work', 'primarily', 'required', 'teachers', 'to', 'conform', 'to', 'long-established', 'practices,', 'routines,', 'and', 'protocols.This', 'approach', 'was', 'the', 'foundation', 'of', 'the', 'process-based,', 'factory', 'mindset', 'design', 'exhibited', 'by', 'the', 'traditional', 'education', 'system.However,', 'even', 'before', 'the', 'appearance', 'of', 'COVID-19,', 'there', 'had', 'been', 'a', 'dramatic', 'shift', 'to', 'a', 'post-industrial,', 'global', 'digital', 'world', 'and', 'economy,', 'which', 'led', 'to', 'additional', 'changes', 'to', 'long-held', 'values,', 'expectations,', 'and', 'assumptions.As', 'a', 'result,', 'we', 'often', 'sought', 'technical', 'solutions', 'to', 'solve', 'adaptive', 'challenges', 'in', 'our', 'schools', '-', 'something', 'COVID-19', 'has', 'further', 'accelerated.Moving', 'forward', '(hopefully)', 'post-pandemic,', 'the', 'primary', 'challenges', 'facing', 'educators', 'will', 'be', 'more', 'adaptive', 'than', 'technical.With', 'today’s', 'COVID', 'learners,', 'we', 'are', 'increasingly', 'facing', 'drop-out,', 'tune-out', 'issues.In', 'a', 'world', 'full', 'of', 'accelerating', 'levels', 'of', 'distraction', 'and', 'choice,', 'there', 'are', 'any', 'number', 'of', 'alternatives', 'to', 'engaging', 'in', 'school', '-', 'many', 'of', 'which', 'have', 'been', 'further', 'amplified', 'by', 'the', 'effects', 'of', 'the', 'pandemic.']\n"
     ]
    }
   ],
   "source": [
    "x = chunks_data[5]\n",
    "print(len(x.split(\" \")),x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_beginners = [\n",
    "    # ''' Analyse  the following text  weather it has number_treatment give me respose in \"Yes\" or \"No\" only'''\n",
    "    ''' count no of phonetics, symbols that are there in the doccument and give me the count number '''\n",
    "    # '''Analyse  the following text  weather it has Single_or_double_quotes and give me respose in \"Single\" or \"double\" only?:''',\n",
    "    # '''Analyse  the following text  weather it has Series_comma and give me respose in \"Yes\" or \"No\" only?:''',\n",
    "    # '''analyse  the below text  weather it is in \"American_english style\" or \"British english style and give me respose in \"American\" or \"British\" only?:\"'''\n",
    "]\n",
    "\n",
    "Modified_chunks = []\n",
    "for prompt in prompt_beginners:\n",
    "    prompt_line = f'''{prompt}'''\n",
    "    for chunk_i in chunks_data[:10]:\n",
    "        input_text = prompt_line + chunk_i\n",
    "        Modified_chunks.append(input_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(Modified_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' count no of phonetics, symbols are there in the doccument and give me the count number Library of Congress Cataloging-in-Publication Data\\nA catalog record for this title has been requested\\n\\nISBN: 9780367567910 (hbk)\\nISBN: 9780367610340 (pbk)\\nISBN: 9781003102984 (ebk)\\n\\nDOI: 10.4324/9781003102984\\n\\nTypeset in Palatino \\nby [Typesetter]\\n\\n\\nDedications\\n\\nI want to dedicate this book to my family.Rachel, Connor, and Ben, you are my life.To my wonderful mother, Susan, and my sister, Kristy, - thank you for the love and support.I also would like to thank the faculty, staff, and students at both Notre Dame of Maryland University and Johns Hopkins University for allowing me to fulfill my passion for teaching the teachers of today and tomorrow.-Ryan L.Schaaf\\n\\nI want to dedicate this book to my parents, who always promoted the value of a strong education.From installing my pretend classroom in our basement and pretending to be my first students, to encouraging me throughout college and graduate school, I am forever grateful.To my husband, Nick, thank you for being my first editor and always allowing me to bounce ideas off of you.We are a team.Thank you for always supporting me.Ian and Ryan, thank you for this unbelievable opportunity and all that you have taught me throughout this experience.-Becky Zayas\\n\\nThis book is intended to celebrate the exceptional dedication and courage educators have exhibited, and to acknowledge their demonstrated capacity to adapt and innovate in extraordinarily challenging and uncertain conditions.Now is the time for us to recognize the exceptional role they play, and to empower them with the training, professional development, support, and working conditions needed to effectively deploy their talents.For the education system to recover from the COVID pandemic requires sustained investment in the well-being, training, professional development and working conditions of the world’s 71 million educators.Education recovery will only be successful if it is conducted hand-in-hand with teachers, giving them both voice and agency to participate in the critical change process.- Ian Jukes\\n\\n\\n\\n\\nContents\\n\\nMeet the Authors\\n\\nForeword: Dr.Nicky Mohan, InfoSavvy 21\\n\\nIntroduction: The Need for Teacher and Learner Agency In Pandemic Times \\nStorytime \\nA Starting Point \\nChapter 1: What is Learner Agency?Chapter 2: The Empowerers of Empowerment \\n\\nChapter 3: The Digital Generations and the Great Disconnect \\n\\nChapter 4: Rise of the Creative Class \\n\\nChapter 5: Modern Learners, Modern Skills \\n\\nChapter 6: THE LIST \\n\\nChapter 7: Authentic Assessment for Authentic Learning \\n\\nChapter 8: The Best of Both Worlds: Providing Learner Empowerment in the Age of High-Stakes Learning\\n\\n\\nMeet the Authors\\n[Insert 15031-4983-0FM-Figure-001 Here]\\nRyan L.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Modified_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Modified_chunks[2]\n",
    "# print(len(x.split(\" \")),x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation List: ['648ffe58a4b17db3cc6b83fb', '648ffe65bdb2191d2c0a979f']\n",
      "Response: There are approximately 30 phonetic symbols in the document. These include letters with diacritic marks, such as á, é, í, ó, ú, ü, ñ, and accented vowels like à, è, ì, ò, ù, û, ï. Additionally, there are symbols representing Greek letters, such as α, β, γ, δ, ε, ζ, η, θ, ι, κ, λ, μ, ν, ξ, ο, π, ρ, σ, τ, υ, φ, χ, ψ, ω. Finally, there are symbols representing mathematical operators, such as +, -, ×, ÷, =, <, >, (, ), ^, _, etc.\n",
      "\n",
      "Conversation List: ['648ffe6c4d344ab119af4319', '648ffe764571852aa49e0347']\n",
      "Response: There are 14 phonetic symbols in the given document. These include \"@\" symbol used as part of email addresses, \"#\" symbol used in chapter titles, \"%\" symbol used in URLs, \"^\" symbol used in superscript text, \"&\" symbol used in names of companies, \"'\" symbol used in quotations, \"<>\" symbol used in HTML code, \"( )\" symbol used in parentheses, \"-\" symbol used in hyphenation, \":\" symbol used in times and dates, \";\" symbol used in lists, \".\" symbol used in decimal numbers, and \",\" symbol used in lists.\n",
      "\n",
      "Conversation List: ['648ffe7e4571852aa49e0348', '648ffe81724ffa15d6623c80']\n",
      "Response: There are 6 phonetic symbols in this document.\n",
      "\n",
      "Conversation List: ['648ffe89525d2d2474970944', '648ffe90a4b17db3cc6b83fd']\n",
      "Response: There are approximately 54 phonetic symbols in this document. These include letters, numbers, punctuation marks, and other characters used for formatting and layout purposes. However, if you only count the actual words and sentences, then there would be around 700-800 phonetic symbols.\n",
      "\n",
      "Conversation List: ['648ffe98724ffa15d6623c84', '648ffe9b4571852aa49e034c']\n",
      "Response: There are 26 phonetic symbols in the document.\n",
      "\n",
      "Conversation List: ['648ffea24571852aa49e034d', '648ffea44571852aa49e034e']\n",
      "Response: 20\n",
      "\n",
      "Conversation List: ['648ffeabbdb2191d2c0a97a5', '648ffeae724ffa15d6623c85']\n",
      "Response: There are 12 phonetic symbols in the given document.\n",
      "\n",
      "Conversation List: ['648ffeb5525d2d2474970947', '648ffeb7bdb2191d2c0a97a6']\n",
      "Response: There are 12 phonetic symbols in the given document.\n",
      "\n",
      "Conversation List: ['648ffebea4b17db3cc6b83fe', '648ffec14571852aa49e0350']\n",
      "Response: There are 10 phonetic symbols in the document.\n",
      "\n",
      "Conversation List: ['648ffec8bdb2191d2c0a97a7', '648ffeca56b64bb432151de0']\n",
      "Response: There are 14 phonetic symbols in the given document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk_data in Modified_chunks[:]:\n",
    "    conversation_list, response = chat_with_bot(chunk_data)\n",
    "    print(\"Conversation List:\", conversation_list)\n",
    "    print(\"Response:\", response)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Modified_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSingle_or_double_quotes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeries_comma\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmerican_english\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_treatment\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m punctuational_error_count, grammatical_error_count, spelling_error_count,missing_articles_count \u001b[38;5;241m=\u001b[39m[],[],[],[]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mModified_chunks\u001b[49m[:]:\n\u001b[1;32m      5\u001b[0m     conversation_list, response \u001b[38;5;241m=\u001b[39m chat_with_bot(chunk_data)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversation List:\u001b[39m\u001b[38;5;124m\"\u001b[39m, conversation_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Modified_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Single_or_double_quotes', 'Series_comma','American_english',\"number_treatment\"])\n",
    "\n",
    "punctuational_error_count, grammatical_error_count, spelling_error_count,missing_articles_count =[],[],[],[]\n",
    "for chunk_data in Modified_chunks[:]:\n",
    "    conversation_list, response = chat_with_bot(chunk_data)\n",
    "    print(\"Conversation List:\", conversation_list)\n",
    "    print(\"Response:\", response)\n",
    "    print()\n",
    "\n",
    "    # Extract the numeric value using regular expressions\n",
    "    # error_count = re.findall(r'\\d+', response.split(\"\\n\")[0])\n",
    "\n",
    "    if error_count:\n",
    "        # Convert the extracted value to an integer\n",
    "        error_count = int(error_count[0])\n",
    "    else:\n",
    "        # If a numeric value is not found, try to convert words to numbers\n",
    "        try:\n",
    "            error_count = response.split(\"\\n\")[0]\n",
    "        except ValueError:\n",
    "            error_count = 0\n",
    "    \n",
    "    print(\"error_count:\", response)\n",
    "\n",
    "    # Determine the column to assign the error count based on the prompt\n",
    "    if \"Single_or_double_quotes\" in chunk_data:\n",
    "        # column_name = 'punctuational error_count'\n",
    "        punctuational_error_count.append(response)\n",
    "    if \"Series_comma\" in chunk_data:\n",
    "        # column_name = 'grammatical error_count'\n",
    "        grammatical_error_count.append(response)\n",
    "    if \"American_english \" in chunk_data:\n",
    "        # column_name = 'spelling_error_count'\n",
    "        spelling_error_count.append(response)\n",
    "    if \"number_treatment\" in chunk_data:\n",
    "        # column_name = 'missing_articles_count'\n",
    "        missing_articles_count.append(error_count)\n",
    "    else:\n",
    "        column_name = None\n",
    "\n",
    "    # print(\"column_name: \", column_name)\n",
    "\n",
    "    # # Add the error count to the dataframe if it is not None and column_name is valid\n",
    "    # if error_count is not None and column_name:\n",
    "    #     if column_name in df.columns:\n",
    "    #         df[column_name] = df.get(column_name, 0) + error_count\n",
    "    #     else:\n",
    "    #         df[column_name] = error_count\n",
    "\n",
    "print(\"punc\",punctuational_error_count,len(punctuational_error_count))\n",
    "print(\"Grammer\",grammatical_error_count,len(grammatical_error_count))\n",
    "print(\"spelling\",spelling_error_count,len(spelling_error_count))\n",
    "# print(\"arti\",missing_articles_count,len(missing_articles_count))\n",
    "df = pd.DataFrame({'punctuational error_count':punctuational_error_count,\n",
    "                   'grammatical error_count':grammatical_error_count, \n",
    "                   'spelling_error_count':spelling_error_count})\n",
    "                #    'missing_articles_count':missing_articles_count})\n",
    "# Print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single_or_double_quotes</th>\n",
       "      <th>Series_comma</th>\n",
       "      <th>American_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>American English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>single</td>\n",
       "      <td>Yes</td>\n",
       "      <td>British English:\\n\\n* For some, this distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English:\\nThe text appears to be writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>American English:\\n\\nThis text appears to be w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>American English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Single_or_double_quotes Series_comma  \\\n",
       "0                  double          Yes   \n",
       "1                  double          Yes   \n",
       "2                  double          Yes   \n",
       "3                  double          Yes   \n",
       "4                  double         Yes.   \n",
       "5                  double          Yes   \n",
       "6                  single          Yes   \n",
       "7                  double          Yes   \n",
       "8                  double         Yes.   \n",
       "9                  double         Yes.   \n",
       "\n",
       "                                    American_english  \n",
       "0                            American English style.  \n",
       "1                            American English style.  \n",
       "2                            American English style.  \n",
       "3                                  American English.  \n",
       "4                                   American English  \n",
       "5                                                  0  \n",
       "6  British English:\\n\\n* For some, this distracti...  \n",
       "7  American English:\\nThe text appears to be writ...  \n",
       "8  American English:\\n\\nThis text appears to be w...  \n",
       "9                                   American English  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df.rename(columns={\"punctuational error_count\": \"Single_or_double_quotes\", \"grammatical error_count\": \"Series_comma\",\"American_english\":\"Style\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single_or_double_quotes</th>\n",
       "      <th>Series_comma</th>\n",
       "      <th>Style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English style.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes</td>\n",
       "      <td>American English.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>double</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>American English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Single_or_double_quotes Series_comma                    Style\n",
       "0                  double          Yes  American English style.\n",
       "1                  double          Yes  American English style.\n",
       "2                  double          Yes  American English style.\n",
       "3                  double          Yes        American English.\n",
       "4                  double         Yes.         American English"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
