{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:27:45.674948: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-20 12:27:45.732868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 12:27:46.498104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "# import pypdfium2 as pdfium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pytesseract\n",
    "import pytorch_lightning as pl\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from imutils import resize\n",
    "from pdf2image import convert_from_bytes, convert_from_path\n",
    "from PIL import Image\n",
    "from PIL import JpegImagePlugin as jplugin\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import (Concatenate, Conv2D, Conv2DTranspose,\n",
    "                                     Dropout, Input, Layer, UpSampling2D)\n",
    "from tensorflow.keras.models import Model\n",
    "from torchvision import models, transforms\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "labels = 0\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global Classificationmodel\n",
    "class Classificationmodel(pl.LightningModule):\n",
    "    \"\"\"Core model where it takes resnet18 by default\n",
    "    creates a training step, validation step and\n",
    "    test step. it also has configure optimizers\"\"\"\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(Classificationmodel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.classes = classes\n",
    "        self.modelname = \"resnet\"\n",
    "        self.model = self.build_model()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = 0.001\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"returns the model based on the model name along\n",
    "        with number of classes of dataset as a final layer\n",
    "        in model\"\"\"\n",
    "        if self.modelname in [\"resnet\", \"inception\", \"googlenet\"]:\n",
    "            infeatures = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(infeatures, self.classes)\n",
    "        if self.modelname in [\"vggnet\", \"alexnet\"]:\n",
    "            infeatures = self.model.classifier[6].in_features\n",
    "            self.model.classifier[6] = nn.Linear(infeatures, self.classes)\n",
    "        if self.modelname in [\"mobilenet\"]:\n",
    "            infeatures = self.model.classifier[1].in_features\n",
    "            self.model.classifier[1] = nn.Linear(infeatures, self.classes)\n",
    "        if self.modelname in [\"densenet\"]:\n",
    "            infeatures = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(infeatures, self.classes)\n",
    "        return self.model\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        accuracy = self.accuracy(output, y)\n",
    "        self.log(\"train_acc_step\", accuracy)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        print(\"train_loss\", loss)\n",
    "        print(\"train accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        accuracy = self.accuracy(output, y)\n",
    "        self.log(\"val_acc_step\", accuracy)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # print(\"val_loss\", loss)\n",
    "        # print(\"val accuracy\", accuracy)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        self.accuracy(output, y)\n",
    "        self.log(\"test_acc_step\", self.accuracy)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # if self.optim.lower() == 'sgd':\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        # if self.optim.lower() == 'adam':\n",
    "        # optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TableDetectionInImage:\n",
    "    def __init__(self):\n",
    "        self.path = \"model_15.h5\"\n",
    "        self.table_detect_model()\n",
    "\n",
    "    ### model architecture for the table detection\n",
    "    def table_detect_model(self):\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        class table_mask(Layer):\n",
    "            def __init__(self, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.conv_7 = Conv2D(\n",
    "                    kernel_size=(1, 1),\n",
    "                    filters=128,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                )\n",
    "                self.upsample_pool4 = UpSampling2D(\n",
    "                    size=(2, 2), interpolation=\"bilinear\"\n",
    "                )\n",
    "                self.upsample_pool3 = UpSampling2D(\n",
    "                    size=(2, 2), interpolation=\"bilinear\"\n",
    "                )\n",
    "                self.upsample_final = Conv2DTranspose(\n",
    "                    filters=2,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"softmax\",\n",
    "                )\n",
    "\n",
    "            def call(self, input, pool3, pool4):\n",
    "                x = self.conv_7(input)\n",
    "                x = self.upsample_pool4(x)\n",
    "                x = Concatenate()([x, pool4])\n",
    "\n",
    "                x = self.upsample_pool3(x)\n",
    "                x = Concatenate()([x, pool3])\n",
    "\n",
    "                x = UpSampling2D((2, 2))(x)\n",
    "                x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "                x = self.upsample_final(x)\n",
    "                return x\n",
    "\n",
    "        class col_mask(Layer):\n",
    "            def __init__(self, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.conv_7 = Conv2D(\n",
    "                    kernel_size=(1, 1),\n",
    "                    filters=128,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.004),\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "                self.drop = Dropout(0.8)\n",
    "                self.conv_8 = Conv2D(\n",
    "                    kernel_size=(1, 1),\n",
    "                    filters=128,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.004),\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "                self.upsample_pool4 = UpSampling2D(\n",
    "                    size=(2, 2), interpolation=\"bilinear\"\n",
    "                )\n",
    "                self.upsample_pool3 = UpSampling2D(\n",
    "                    size=(2, 2), interpolation=\"bilinear\"\n",
    "                )\n",
    "                self.upsample_final = Conv2DTranspose(\n",
    "                    filters=2,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"softmax\",\n",
    "                )\n",
    "\n",
    "            def call(self, input, pool3, pool4):\n",
    "                x = self.conv_7(input)\n",
    "                x = self.drop(x)\n",
    "                x = self.conv_8(x)\n",
    "\n",
    "                x = self.upsample_pool4(x)\n",
    "                x = Concatenate()([x, pool4])\n",
    "\n",
    "                x = self.upsample_pool3(x)\n",
    "                x = Concatenate()([x, pool3])\n",
    "\n",
    "                x = UpSampling2D((2, 2))(x)\n",
    "                x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "                x = self.upsample_final(x)\n",
    "                return x\n",
    "\n",
    "        class F1_Score(tf.keras.metrics.Metric):\n",
    "            def __init__(self, name=\"f1_score\", **kwargs):\n",
    "                super().__init__(name=name, **kwargs)\n",
    "                self.f1 = self.add_weight(name=\"f1\", initializer=\"zeros\")\n",
    "                self.precision_fn = tf.keras.metrics.Precision(thresholds=0.5)\n",
    "                self.recall_fn = tf.keras.metrics.Recall(thresholds=0.5)\n",
    "\n",
    "            def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "                p = self.precision_fn(y_true, tf.argmax(y_pred, axis=-1))\n",
    "                r = self.recall_fn(y_true, tf.argmax(y_pred, axis=-1))\n",
    "                # since f1 is a variable, we use assign\n",
    "                self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\n",
    "\n",
    "            def result(self):\n",
    "                return self.f1\n",
    "\n",
    "            def reset_states(self):\n",
    "                # we also need to reset the state of the precision and recall objects\n",
    "                self.precision_fn.reset_states()\n",
    "                self.recall_fn.reset_states()\n",
    "                self.f1.assign(0)\n",
    "\n",
    "        input_shape = (1024, 1024, 3)\n",
    "        input_ = Input(shape=input_shape)\n",
    "\n",
    "        vgg19_ = VGG19(\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",\n",
    "            input_tensor=input_,\n",
    "            input_shape=None,\n",
    "            pooling=None,\n",
    "            classes=1000,\n",
    "            classifier_activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "        for layer in vgg19_.layers[:15]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        pool3 = vgg19_.get_layer(\"block3_pool\").output\n",
    "        pool4 = vgg19_.get_layer(\"block4_pool\").output\n",
    "\n",
    "        conv_1_1_1 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(1, 1),\n",
    "            activation=\"relu\",\n",
    "            name=\"block6_conv1\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.004),\n",
    "        )(vgg19_.output)\n",
    "        conv_1_1_1_drop = Dropout(0.8)(conv_1_1_1)\n",
    "\n",
    "        conv_1_1_2 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(1, 1),\n",
    "            activation=\"relu\",\n",
    "            name=\"block6_conv2\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.004),\n",
    "        )(conv_1_1_1_drop)\n",
    "        conv_1_1_2_drop = Dropout(0.8)(conv_1_1_2)\n",
    "\n",
    "        table_mask_output = table_mask()(conv_1_1_2_drop, pool3, pool4)\n",
    "        col_mask_output = col_mask()(conv_1_1_2_drop, pool3, pool4)\n",
    "\n",
    "        model = Model(input_, [table_mask_output, col_mask_output])\n",
    "\n",
    "        losses = {\n",
    "            \"table_mask\": \"sparse_categorical_crossentropy\",\n",
    "            \"col_mask\": \"sparse_categorical_crossentropy\",\n",
    "        }\n",
    "\n",
    "        metrics = [F1_Score(), \"Accuracy\"]\n",
    "\n",
    "        # global init_lr\n",
    "        init_lr = 0.0001\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=init_lr, epsilon=1e-8),\n",
    "            loss=losses,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "\n",
    "        self.detect_model = tf.keras.models.load_model(\n",
    "            self.path,\n",
    "            custom_objects={\n",
    "                \"table_mask\": table_mask,\n",
    "                \"col_mask\": col_mask,\n",
    "                \"F1_Score\": F1_Score,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    ## cropping table images form the table mask\n",
    "    ## converts single page image into table images. individual image for every table that is detected using mode_15.h5\n",
    "    def detected_table_images(self, img_path):\n",
    "        if type(img_path) == Image.Image:\n",
    "            img = np.array(img_path)\n",
    "        elif isinstance(img_path, str):\n",
    "            if \"http\" in img_path:\n",
    "                image = Image.open(requests.get(img_path, stream=True).raw)\n",
    "                img = np.array(image)\n",
    "            else:\n",
    "                image = Image.open(img_path)\n",
    "                img = np.array(image)\n",
    "        elif type(img_path) == jplugin.JpegImageFile:\n",
    "            # img = Image.open(img_path)\n",
    "            img = np.array(img_path)\n",
    "        elif type(img_path) == bytes:\n",
    "            img = Image.open(io.BytesIO(img_path))\n",
    "            img = np.array(img)\n",
    "        else:\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "            except:\n",
    "                imag = Image.open(img_path)\n",
    "                img = np.array(imag)\n",
    "\n",
    "        img = cv2.resize(img, (1024, 1024))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        # Predict table mask\n",
    "        table_mask, _ = self.detect_model.predict(img)\n",
    "\n",
    "        # Threshold mask to get binary image\n",
    "        table_mask = np.argmax(table_mask.squeeze(), axis=-1)\n",
    "        table_mask = np.uint8(table_mask > 0.5)\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        table_mask = cv2.dilate(table_mask, kernel, iterations=1)\n",
    "\n",
    "        # Find contours of connected components\n",
    "        contours, _ = cv2.findContours(\n",
    "            table_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        # Create the new folder if it doesn't exist\n",
    "        # if not os.path.exists(new_folder_path):\n",
    "        #     os.makedirs(new_folder_path)\n",
    "        #     print(\"Created a new folder\")\n",
    "        table_imgs = []   \n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            table_img = img[0][y : y + h, x : x + w]\n",
    "            table_imgs.append(table_img)\n",
    "        return table_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TableImagePreprocessing:\n",
    "    def __init__(self,model):\n",
    "        model_path = r\"model_weights_13.pt\"\n",
    "        # mod = Classificationmodel(3)\n",
    "        self.model = model\n",
    "        self.count = 0\n",
    "\n",
    "    ### rescaling the image\n",
    "    def rescaling(self, image):\n",
    "        image = Image.fromarray(image)\n",
    "        resized_image = image\n",
    "        w = image.size[0]\n",
    "        h = image.size[1]\n",
    "        if min(h, w) < 600:\n",
    "            if w > h or w == h:\n",
    "                factor = 600 / h\n",
    "                h = 600\n",
    "                w = int(factor * w)\n",
    "            else:\n",
    "                factor = 600 / w\n",
    "                w = 600\n",
    "                h = int(factor * h)\n",
    "\n",
    "            resized_image = image.resize((w, h))\n",
    "        return np.asarray(resized_image)\n",
    "\n",
    "    ### removing the background color from the image\n",
    "    def background_removal(self, image_path):\n",
    "        image = image_path.copy()\n",
    "        hsv_img = cv2.cvtColor(\n",
    "            image_path, cv2.COLOR_BGR2HSV\n",
    "        )  # convert image to HSV color space\n",
    "        saturation_scale = 15  # increase saturation\n",
    "        hsv_img[..., 1] = cv2.convertScaleAbs(\n",
    "            hsv_img[..., 1], alpha=saturation_scale, beta=0\n",
    "        )\n",
    "        adjusted_saturation = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n",
    "        hsv = cv2.cvtColor(adjusted_saturation, cv2.COLOR_BGR2HSV)\n",
    "        lower_red = np.array([0, 150, 50])\n",
    "        upper_red = np.array([10, 255, 255])\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        back_ground = []\n",
    "        if len(contours) > 0:\n",
    "            back_ground.append(1)\n",
    "        else:\n",
    "            back_ground.append(2)\n",
    "\n",
    "        if 1 in back_ground:\n",
    "            img = image_path.copy()\n",
    "            hsv_img = cv2.cvtColor(\n",
    "                img, cv2.COLOR_BGR2HSV\n",
    "            )  # convert image to HSV color space\n",
    "            saturation_scale = 0  # increase saturation\n",
    "            hsv_img[..., 1] = cv2.convertScaleAbs(\n",
    "                hsv_img[..., 1], alpha=saturation_scale, beta=0\n",
    "            )\n",
    "            adjusted_saturation = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n",
    "            alpha = 1.4  # increase contrast\n",
    "            beta = 0.5  # no brightness adjustment\n",
    "            adjusted_contrast = cv2.convertScaleAbs(\n",
    "                adjusted_saturation, alpha=alpha, beta=beta\n",
    "            )\n",
    "            return adjusted_contrast\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    ### removing the horizontal and vertical lines form the image for partially bordered table\n",
    "    def line_removal(self, image, original_image):\n",
    "        pil_og_image = Image.fromarray(original_image)\n",
    "        w, h = pil_og_image.size[0], pil_og_image.size[1]\n",
    "        pil_image = Image.fromarray(image)\n",
    "        w1, h1 = pil_image.size[0], pil_image.size[1]\n",
    "\n",
    "        img = image.copy()\n",
    "        img = cv2.resize(img, (w, h))\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (1, 1), 1)\n",
    "        _, thresh = cv2.threshold(\n",
    "            blur, 50, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU\n",
    "        )\n",
    "\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        thresh1 = cv2.dilate(thresh, kernel=kernel, iterations=1)\n",
    "        kernel1 = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 70))\n",
    "        opening1 = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel1, iterations=1)\n",
    "\n",
    "        result = cv2.bitwise_and(thresh, cv2.bitwise_not(opening1))\n",
    "        result1 = cv2.cvtColor(cv2.bitwise_not(result), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        gray = cv2.cvtColor(result1, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        low_threshold = 50\n",
    "        high_threshold = 100\n",
    "        edges = cv2.Canny(blur, low_threshold, high_threshold)\n",
    "\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi / 20, 350)  # 20, 350\n",
    "        if lines is not None:\n",
    "            for line in lines:\n",
    "                rho, theta = line[0]\n",
    "                a = np.cos(theta)\n",
    "                b = np.sin(theta)\n",
    "                x0 = a * rho\n",
    "                y0 = b * rho\n",
    "                x1 = int(x0 + 1000 * (-b))\n",
    "                y1 = int(y0 + 1000 * (a))\n",
    "                x2 = int(x0 - 1000 * (-b))\n",
    "                y2 = int(y0 - 1000 * (a))\n",
    "\n",
    "                cv2.line(result1, (x1, y1), (x2, y2), (255, 255, 255), 4)\n",
    "\n",
    "        img = cv2.resize(result1, (w1, h1))\n",
    "        return img\n",
    "\n",
    "    def sort_contours(self, cnts, method=\"left-to-right\"):\n",
    "        \"\"\"Return sorted countours.\"\"\"\n",
    "        reverse = False\n",
    "        k = 0\n",
    "        if method in [\"right-to-left\", \"bottom-to-top\"]:\n",
    "            reverse = True\n",
    "        if method in [\"top-to-bottom\", \"bottom-to-top\"]:\n",
    "            k = 1\n",
    "        b_boxes = [cv2.boundingRect(c) for c in cnts]\n",
    "        (cnts, b_boxes) = zip(\n",
    "            *sorted(zip(cnts, b_boxes), key=lambda b: b[1][k], reverse=reverse)\n",
    "        )\n",
    "        return (cnts, b_boxes)\n",
    "\n",
    "    ## drawing  the horizontal and vertical lines\n",
    "    def draw_lines(self, tbl_image):\n",
    "        # tbl_image = img\n",
    "        tbl_gray = cv2.cvtColor(tbl_image, cv2.COLOR_BGR2GRAY)\n",
    "        tbl_thresh_bin = cv2.threshold(tbl_gray, 225, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        R = 3.0\n",
    "        tbl_resized = resize(tbl_thresh_bin, width=int(tbl_image.shape[1] // R))\n",
    "\n",
    "        def get_dividers(img, axis):\n",
    "            \"\"\"Return array indicies of white horizontal or vertical lines.\"\"\"\n",
    "            blank_lines = np.where(np.all(img == 255, axis=axis))[0]\n",
    "            filtered_idx = np.where(np.diff(blank_lines) != 1)[0]\n",
    "            if axis == 0:\n",
    "                c = 0\n",
    "                filtered_idx_1 = []\n",
    "                for i in range(len(filtered_idx)):\n",
    "                    if i == 0:\n",
    "                        filtered_idx_1.append(filtered_idx[i])\n",
    "                        continue\n",
    "                    if filtered_idx[i] - filtered_idx[c] > 5:\n",
    "                        filtered_idx_1.append(filtered_idx[i])\n",
    "                    c = i\n",
    "                filtered_idx = filtered_idx_1\n",
    "            return blank_lines[filtered_idx]\n",
    "\n",
    "        dims = tbl_image.shape[0], tbl_image.shape[1]\n",
    "        tbl_str = np.zeros(dims, np.uint8)\n",
    "        tbl_str = cv2.rectangle(tbl_str, (0, 0), (dims[1] - 1, dims[0] - 1), 255, 1)\n",
    "\n",
    "        for a in [0, 1]:\n",
    "            dividers = get_dividers(tbl_resized, a)\n",
    "            start_point = [0, 0]\n",
    "            if a == 0:\n",
    "                end_point = [dims[0], dims[0] ]\n",
    "            else:\n",
    "                end_point = [dims[1] , dims[1]]\n",
    "            for i in dividers:\n",
    "                i *= R\n",
    "                start_point[a] = int(i)\n",
    "                end_point[a] = int(i)\n",
    "                cv2.line(tbl_str, tuple(start_point), tuple(end_point), 255, 1)\n",
    "\n",
    "        contours, _ = cv2.findContours(tbl_str, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        contours, boundingBoxes = self.sort_contours(contours, method=\"top-to-bottom\")\n",
    "\n",
    "        # remove countours of the whole table\n",
    "        bb_filtered = [\n",
    "            list(t) for t in boundingBoxes if t[2] < dims[1] and t[3] < dims[0]\n",
    "        ]\n",
    "\n",
    "        # allocate countours in table-like structure\n",
    "        rows = []\n",
    "        columns = []\n",
    "\n",
    "        for i, bb in enumerate(bb_filtered):\n",
    "            if i == 0:\n",
    "                columns.append(bb)\n",
    "                previous = bb\n",
    "            else:\n",
    "                if bb[1] < previous[1] + previous[3] / 2:\n",
    "                    columns.append(bb)\n",
    "                    previous = bb\n",
    "                    if i == len(bb_filtered) - 1:\n",
    "                        rows.append(columns)\n",
    "                else:\n",
    "                    rows.append(columns)\n",
    "                    columns = []\n",
    "                    previous = bb\n",
    "                    columns.append(bb)\n",
    "\n",
    "        for row in rows:\n",
    "            for column in row:\n",
    "                x, y, w, h = column\n",
    "                cv2.rectangle(tbl_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            if len(row) > 0:\n",
    "                x, y, w, h = row[0]\n",
    "        return tbl_image\n",
    "\n",
    "    # detection of horizontal and vertical lines to get the final boxes\n",
    "    #finds the intersection points \n",
    "    def detect_horizontal_vertical_lines(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # last_image_num = 0\n",
    "        # existing_images = [image for image in os.listdir(\"./img/l_img\") if image.startswith(\"line_\")]\n",
    "        # if existing_images:\n",
    "        #     last_image = max(existing_images, key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "        #     last_image_num = int(last_image.split(\"_\")[1].split(\".\")[0])\n",
    "        # # Increment the image number\n",
    "        # new_image_num = last_image_num + 1\n",
    "        # new_image_name = f\"./img/l_img/line_{new_image_num}.png\"\n",
    "\n",
    "        # print(\"lines image saved as -\",new_image_name)\n",
    "\n",
    "        # # Create the new folder if it doesn't exist\n",
    "        # if not os.path.exists(new_folder_path):\n",
    "        #     os.makedirs(new_folder_path)\n",
    "        #     print(\"created a new folder\")\n",
    "\n",
    "        # cv2.imwrite(new_image_name,img)\n",
    "        # # image.save(os.path.join(new_folder_path, f\"image{num}.png\"))\n",
    "        # thresholding the image to a binary image\n",
    "        _, img_bin = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        # inverting the image\n",
    "        img_bin = 255 - img_bin\n",
    "        # Length(width) of kernel as 100th of total width\n",
    "        kernel_len = np.array(img).shape[1] // 100\n",
    "        # Defining a vertical kernel to detect all vertical lines of image\n",
    "        ver_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_len))\n",
    "        # Defining a horizontal kernel to detect all horizontal lines of image\n",
    "        hor_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_len, 1))\n",
    "        # A kernel of 2x2\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "\n",
    "        image_1 = cv2.erode(img_bin, ver_kernel, iterations=3)\n",
    "        vertical_lines = cv2.dilate(image_1, ver_kernel, iterations=3)\n",
    "        image_2 = cv2.erode(img_bin, hor_kernel, iterations=3)\n",
    "        horizontal_lines = cv2.dilate(image_2, hor_kernel, iterations=3)\n",
    "\n",
    "        img_vh = cv2.addWeighted(vertical_lines, 0.1, horizontal_lines, 0.1, 0.0)\n",
    "        # Eroding and thesholding the image\n",
    "        img_vh = cv2.erode(~img_vh, kernel, iterations=2)\n",
    "        _, img_vh = cv2.threshold(img_vh, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        bitxor = cv2.bitwise_xor(img, img_vh)\n",
    "        bitnot = cv2.bitwise_not(bitxor)\n",
    "\n",
    "        contours, _ = cv2.findContours(img_vh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort all the contours by top to bottom.\n",
    "        contours, boundingBoxes = self.sort_contours(contours, method=\"top-to-bottom\")\n",
    "\n",
    "        heights = [boundingBoxes[i][3] for i in range(len(boundingBoxes))]\n",
    "        # Get mean of heights\n",
    "        mean = np.mean(heights)\n",
    "\n",
    "        box = []\n",
    "        # Get position (x,y), width and height for every contour and show the contour on image\n",
    "        for c in contours:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "            if img.shape[0] < 700 and img.shape[1] < 1000:\n",
    "                if w < 1000 and h < 500:\n",
    "                    image = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    box.append([x, y, w, h])\n",
    "            elif img.shape[0] < 700 and img.shape[1] > 1000:\n",
    "                if w < 2100 and h < 500:\n",
    "                    image = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    box.append([x, y, w, h])\n",
    "            elif img.shape[0] < 1000 and img.shape[1] > 700:\n",
    "                if w < 500 and h < 2100:\n",
    "                    image = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    box.append([x, y, w, h])\n",
    "            else:\n",
    "                if w < 2100 and h < 1000:\n",
    "                    image = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    box.append([x, y, w, h])\n",
    "\n",
    "        row = []\n",
    "        column = []\n",
    "        j = 0\n",
    "        for i in range(len(box)):\n",
    "            if i == 0:\n",
    "                column.append(box[i])\n",
    "                previous = box[i]\n",
    "            else:\n",
    "                if box[i][1] <= previous[1] + mean / 2:\n",
    "                    column.append(box[i])\n",
    "                    previous = box[i]\n",
    "                    if i == len(box) - 1:\n",
    "                        row.append(column)\n",
    "                else:\n",
    "                    row.append(column)\n",
    "                    column = []\n",
    "                    previous = box[i]\n",
    "                    column.append(box[i])\n",
    "\n",
    "        countcol = 0\n",
    "        for i in range(len(row)):\n",
    "            countcol = len(row[i])\n",
    "            if countcol > countcol:\n",
    "                countcol = countcol\n",
    "\n",
    "            center = [\n",
    "                int(row[i][j][0] + row[i][j][2] / 2)\n",
    "                for j in range(len(row[i]))\n",
    "                if row[0]\n",
    "            ]\n",
    "            center = np.array(center)\n",
    "            center.sort()\n",
    "\n",
    "        finalboxes = []\n",
    "        for i in range(len(row)):\n",
    "            ls = []\n",
    "            for k in range(countcol):\n",
    "                ls.append([])\n",
    "            for j in range(len(row[i])):\n",
    "                diff = abs(center - (row[i][j][0] + row[i][j][2] / 4))\n",
    "                minimum = min(diff)\n",
    "                indexing = list(diff).index(minimum)\n",
    "                ls[indexing].append(row[i][j])\n",
    "            finalboxes.append(ls)\n",
    "\n",
    "        return self.ocr(finalboxes, bitnot, row, countcol)\n",
    "\n",
    "    #### ocr from the finalboxes\n",
    "    def ocr(self, finalboxes, bitnot, row, countcol):\n",
    "        outer = []\n",
    "        sum = 0 \n",
    "        for i in range(len(finalboxes)):\n",
    "            for j in range(len(finalboxes[i])):\n",
    "                inner = \"\"\n",
    "                if len(finalboxes[i][j]) == 0:\n",
    "                    outer.append(\" \")\n",
    "                else:\n",
    "                    for k in range(len(finalboxes[i][j])):\n",
    "                        y, x, w, h = (\n",
    "                            finalboxes[i][j][k][0],\n",
    "                            finalboxes[i][j][k][1],\n",
    "                            finalboxes[i][j][k][2],\n",
    "                            finalboxes[i][j][k][3],\n",
    "                        )\n",
    "                        finalimg = bitnot[x : x + h, y : y + w]\n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))\n",
    "                        border = cv2.copyMakeBorder(\n",
    "                            finalimg, 2, 2, 2, 2, cv2.BORDER_CONSTANT, value=[255, 255]\n",
    "                        )\n",
    "                        resizing = cv2.resize(\n",
    "                            border, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC\n",
    "                        )\n",
    "                        # dilation = cv2.dilate(resizing, kernel, iterations=1)\n",
    "                        erosion = cv2.erode(resizing, kernel, iterations=1)\n",
    "                        \n",
    "                        # cv2.imwrite(f\"./e_img/er_img{sum}.png\", erosion)\n",
    "                        sum += 1 \n",
    "                        out = pytesseract.image_to_string(erosion, config=\"--psm 6\")\n",
    "                        if len(out) == 0:\n",
    "                            out = pytesseract.image_to_string(erosion, config=\"--psm 3\")\n",
    "                        inner = inner + \" \" + out\n",
    "                        inner = inner.encode(\"utf-8\").decode(\"utf-8\")\n",
    "                        inner = inner.encode(\"utf-8\").decode(\"unicode_escape\")\n",
    "                    outer.append(inner)\n",
    "\n",
    "        arr = np.array(outer)\n",
    "        dataframe = pd.DataFrame(arr.reshape(len(row), countcol))\n",
    "        replace_func = lambda x: x.replace(\"\\n\", \" \").replace(\"\\f\", \" \")\n",
    "        dataframe = dataframe.applymap(replace_func)\n",
    "        dataframe.replace(r\"^\\s*$\", np.nan, regex=True, inplace=True)\n",
    "\n",
    "        dataframe.dropna(axis=1, how=\"all\", inplace=True)\n",
    "        c = 0\n",
    "        for i in dataframe.copy().columns.to_list():\n",
    "            dataframe.rename({i: str(c)}, axis=1, inplace=True)\n",
    "            c += 1\n",
    "\n",
    "        dataframe.dropna(axis=0, how=\"all\", inplace=True)\n",
    "        # data_dict = dataframe.to_dict(orient=\"records\")\n",
    "        dataframe.replace(np.nan, \"\", regex=True, inplace=True)\n",
    "        # Format dictionary as JSON string\n",
    "        return dataframe  # json.dumps({\"Table 1\": data_dict}, indent=4)\n",
    "\n",
    "    def is_clean_image(self, img, threshold_contours=100):\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        thresh_img = cv2.threshold(gray_img, 50, 255, cv2.THRESH_OTSU)[1]\n",
    "        contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) > threshold_contours:\n",
    "            return img\n",
    "\n",
    "    ### prediction for the classification model\n",
    "    def table_border_classification_and_identification(self, image_path):\n",
    "        image = Image.fromarray(image_path.astype(\"uint8\"), \"RGB\")\n",
    "        image.save(\"./table_img.png\")\n",
    "        width, height = image.size\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Resize((224, 224)), transforms.ToTensor()]\n",
    "        )\n",
    "        t_img = transform(image).unsqueeze(0)\n",
    "        self.model.eval()\n",
    "        out = self.model(t_img)\n",
    "        # out = model(t_img)\n",
    "        _, pred = out.max(1)\n",
    "        # print(\"Predicted\")\n",
    "        class_dict = {\n",
    "            0: \"bordered_table\",\n",
    "            1: \"borderless_table\",\n",
    "            3: \"partially_bordered_table\",\n",
    "            2: \"others\"\n",
    "        }\n",
    "        result = class_dict[pred.item()]\n",
    "        print(\"result\",result)\n",
    "        \n",
    "        # if result == \"bordered_table\":\n",
    "        #     image.save(os.path.join(\"./img/bordered_table\", f\"image{self.count}.png\"))\n",
    "        # elif result == \"borderless_table\":\n",
    "        #     if np.mean(image) < 253 and width > 130 and height> 130:\n",
    "        #         print(\"borderless_table mean - \",np.mean(image))\n",
    "        #         image.save(os.path.join(\"./img/borderless_table\", f\"image{self.count}.png\"))\n",
    "        # elif result == \"partially_bordered_table\":\n",
    "        #     image.save(os.path.join(\"./img/partially_bordered_table\", f\"image{self.count}.png\"))\n",
    "        # else:\n",
    "        #     image.save(os.path.join(\"./img/others\", f\"image{self.count}.png\"))\n",
    "        # self.count += 1\n",
    "        \n",
    "        # image.save(\"result.png\")\n",
    "        \n",
    "        img = self.rescaling(image_path)\n",
    "        img = self.background_removal(img)\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        white_pixels = cv2.countNonZero(gray)\n",
    "        total_pixels = img.shape[0] * img.shape[1]\n",
    "        white_percentage = (white_pixels / total_pixels) * 100\n",
    "        # if not a Complete white image, means if has text\n",
    "        # print(\"white_percentage\",white_percentage)\n",
    "        # if not a Complete white image, means if has text and width > 941 and height> 641\n",
    "        \n",
    "        \n",
    "        if white_percentage < 99.99 and np.mean(img) < 253 and width > 130 and height> 130:\n",
    "            # print(\"mean of the image that is saved - \",np.mean(image))\n",
    "            # print(\"width and size \",width,height)\n",
    "            if result == \"partially_bordered_table\":\n",
    "                img = self.line_removal(img, image_path)\n",
    "                img = self.is_clean_image(img)\n",
    "                \n",
    "            if img is not None and result in [\"borderless_table\", \"partially_bordered_table\"]:\n",
    "                img = self.draw_lines(img)\n",
    "                img = self.is_clean_image(img)\n",
    "            \n",
    "            if img is not None:\n",
    "                return self.detect_horizontal_vertical_lines(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = ['bordered_table','borderless_table','partially_bordered_table','others','l_img']\n",
    "# Define the parent directory path where the folders will be created\n",
    "parent_directory = './img'\n",
    "import shutil\n",
    "# Create the folders\n",
    "for folder_name in folder_names:\n",
    "    folder_path = os.path.join(parent_directory, folder_name)\n",
    "    # os.remove(folder_name)\n",
    "    # shutil.rmtree(folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result partially_bordered_table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Â°.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "2     -  \n",
       "4     ~  \n",
       "8   Â°.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(r\"model_weights_13.pt\", map_location=torch.device(\"cpu\"))\n",
    "obj = TableImagePreprocessing(model)\n",
    "imag = cv2.imread('/home/ubuntu/ritesh_manchikanti/pdf-extractor-api/text_ss.png')\n",
    "extraction = obj.table_border_classification_and_identification(imag)\n",
    "extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
