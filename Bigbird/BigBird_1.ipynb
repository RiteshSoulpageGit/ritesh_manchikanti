{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install --quiet sentencepiece\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BigBirdTokenizer,BigBirdModel,BigBirdForSequenceClassification\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_train.parquet')\n",
    "val_df = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_val.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = ''\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a document into smaller chunks\n",
    "def split_document(text, chunk_size):\n",
    "    # Split the text into chunks of size 'chunk_size'\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text of each document\n",
    "encoded_data = []\n",
    "text = extract_text_from_pdf(\"./15031-4983-FullBook.pdf\")\n",
    "encoded_text = tokenizer(text, padding='max_length', truncation=True, max_length=512)\n",
    "encoded_data.append({\n",
    "    'input_ids': encoded_text['input_ids'],\n",
    "    'attention_mask': encoded_text['attention_mask'],\n",
    "    'label': \"1\"  # Replace 'i' with the corresponding index or label for the document\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444130"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90516 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   65,  6289, 41085,  ...,   886,   115,    66]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, add_special_tokens=True, truncation=False, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   65,  6289, 41085,  ...,   886,   115,    66])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "def split_overlapping(tensor: Tensor, chunk_size: int, stride: int, minimal_chunk_length: int) -> list[Tensor]:\n",
    "    \"\"\"Helper function for dividing 1-dimensional tensors into overlapping chunks.\"\"\"\n",
    "    result = [tensor[i : i + chunk_size] for i in range(0, len(tensor), stride)]\n",
    "    if len(result) > 1:\n",
    "        # ignore chunks with less than minimal_length number of tokens\n",
    "        result = [x for x in result if len(x) >= minimal_chunk_length]\n",
    "    return result\n",
    "example_tensor = tokens[\"input_ids\"][0]\n",
    "example_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_into_smaller_chunks(input_id: Tensor,att_mask: Tensor, chunk_size: int, stride: int, minimal_chunk_length: int):\n",
    "    input_id_chunks = [input_id[i : i + chunk_size] for i in range(0, len(input_id), stride)]\n",
    "    mask_chunks = [att_mask[i : i + chunk_size] for i in range(0, len(att_mask), stride)]\n",
    "    if len(input_id_chunks) > 1:\n",
    "        # ignore chunks with less than minimal_length number of tokens\n",
    "        input_id_chunks = [x for x in input_id_chunks if len(x) >= minimal_chunk_length]\n",
    "        mask_chunks = [x for x in mask_chunks if len(x) >= minimal_chunk_length]\n",
    "    return input_id_chunks, mask_chunks\n",
    "\n",
    "\n",
    "def add_special_tokens_at_beginning_and_end(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
    "    \"\"\"\n",
    "    Adds special CLS token (token id = 101) at the beginning.\n",
    "    Adds SEP token (token id = 102) at the end of each chunk.\n",
    "    Adds corresponding attention masks equal to 1 (attention mask is boolean).\n",
    "    \"\"\"\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # adding CLS (token id 101) and SEP (token id 102) tokens\n",
    "        input_id_chunks[i] = torch.cat([Tensor([101]), input_id_chunks[i], Tensor([102])])\n",
    "        # adding attention masks  corresponding to special tokens\n",
    "        mask_chunks[i] = torch.cat([Tensor([1]), mask_chunks[i], Tensor([1])])\n",
    "\n",
    "def add_padding_tokens(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
    "    \"\"\"Adds padding tokens (token id = 0) at the end to make sure that all chunks have exactly 512 tokens.\"\"\"\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # get required padding length\n",
    "        pad_len = 512 - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([input_id_chunks[i], Tensor([0] * pad_len)])\n",
    "            mask_chunks[i] = torch.cat([mask_chunks[i], Tensor([0] * pad_len)])\n",
    "def stack_tokens_from_all_chunks(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> tuple[Tensor, Tensor]:\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "    return input_ids.long(), attention_mask.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_single_text(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    chunk_size: int,\n",
    "    stride: int,\n",
    "    minimal_chunk_length: int,\n",
    "    maximal_text_length,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Transforms (the entire) text to model input of BERT model.\"\"\"\n",
    "    tokens = tokenizer(text, add_special_tokens=True, truncation=False, return_tensors=\"pt\")\n",
    "    # splitted = split_overlapping(tokens[\"input_ids\"][0], chunk_size=5, stride=5, minimal_chunk_length=5)\n",
    "    input_id_chunks, mask_chunks = split_tokens_into_smaller_chunks(tokens[\"input_ids\"][0],tokens[\"attention_mask\"][0], chunk_size, stride, minimal_chunk_length)\n",
    "    add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n",
    "    add_padding_tokens(input_id_chunks, mask_chunks)\n",
    "    input_ids, attention_mask = stack_tokens_from_all_chunks(input_id_chunks, mask_chunks)\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = transform_single_text(text, tokenizer, 510, 510, 1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, attention_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_ids, attention_mask):\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,logits)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2751\u001b[0m, in \u001b[0;36mBigBirdForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2751\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   2752\u001b[0m     input_ids,\n\u001b[1;32m   2753\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2754\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   2755\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   2756\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   2757\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   2758\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2759\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2760\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2761\u001b[0m )\n\u001b[1;32m   2763\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2764\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2043\u001b[0m, in \u001b[0;36mBigBirdModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2043\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m   2044\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m   2046\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#Classify each chunk\n",
    "# tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base')\n",
    "predicted_classes = []\n",
    "for chunk, attention_mask in zip(input_ids, attention_mask):\n",
    "    outputs = model(chunk, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    print(\"Logits\",logits)\n",
    "    probabilities = logits.softmax(dim=1)\n",
    "    predicted_class = probabilities.argmax(dim=1).item()\n",
    "    predicted_classes.append(predicted_class)\n",
    "\n",
    "print(\"Predicted Classes:\", predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong shape for input_ids (shape torch.Size([178, 512])) or attention_mask (shape torch.Size([512]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2112\u001b[0m, in \u001b[0;36mBigBirdModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2109\u001b[0m     to_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2110\u001b[0m     \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   2111\u001b[0m     \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m-> 2112\u001b[0m     extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[1;32m   2113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2114\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2115\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mattention_type can either be original_full or block_sparse, but is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2116\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:902\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[1;32m    900\u001b[0m         extended_attention_mask \u001b[39m=\u001b[39m attention_mask[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m    901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape for input_ids (shape \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m) or attention_mask (shape \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    906\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[39m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong shape for input_ids (shape torch.Size([178, 512])) or attention_mask (shape torch.Size([512]))"
     ]
    }
   ],
   "source": [
    "model_output = model(input_ids,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.1921e-05, 3.7781e-05, 3.1347e-05,  ..., 3.5050e-05,\n",
       "          3.2223e-05, 3.5450e-05],\n",
       "         [1.0023e-03, 8.5958e-04, 8.7710e-04,  ..., 8.4208e-04,\n",
       "          8.0777e-04, 9.0074e-04],\n",
       "         [9.2169e-04, 9.2377e-04, 8.2463e-04,  ..., 7.4543e-04,\n",
       "          8.2347e-04, 8.6752e-04],\n",
       "         ...,\n",
       "         [1.3720e-03, 1.0795e-03, 1.2611e-03,  ..., 9.4047e-04,\n",
       "          1.2315e-03, 1.0444e-03],\n",
       "         [1.5098e-03, 1.0372e-03, 1.2854e-03,  ..., 1.7171e-03,\n",
       "          1.0804e-03, 9.4912e-04],\n",
       "         [7.3206e-04, 4.5095e-04, 5.6538e-04,  ..., 4.7553e-04,\n",
       "          5.8402e-04, 4.2062e-04]],\n",
       "\n",
       "        [[2.4167e-06, 3.0579e-06, 2.6821e-06,  ..., 2.7302e-06,\n",
       "          2.1023e-06, 2.1845e-06],\n",
       "         [1.4922e-03, 1.5710e-03, 1.3152e-03,  ..., 1.0660e-03,\n",
       "          1.1586e-03, 1.0088e-03],\n",
       "         [1.0359e-03, 1.2411e-03, 1.1505e-03,  ..., 1.1374e-03,\n",
       "          1.1750e-03, 8.9946e-04],\n",
       "         ...,\n",
       "         [8.0762e-04, 6.6058e-04, 7.5363e-04,  ..., 1.1364e-03,\n",
       "          9.2742e-04, 8.2603e-04],\n",
       "         [3.5899e-04, 3.6091e-04, 4.6804e-04,  ..., 3.6570e-04,\n",
       "          4.2306e-04, 4.6088e-04],\n",
       "         [1.3803e-03, 1.3018e-03, 1.2120e-03,  ..., 1.3673e-03,\n",
       "          1.3182e-03, 1.4876e-03]],\n",
       "\n",
       "        [[2.7839e-05, 2.1919e-05, 2.0615e-05,  ..., 2.0460e-05,\n",
       "          1.9658e-05, 2.4165e-05],\n",
       "         [1.1158e-03, 1.1814e-03, 1.0175e-03,  ..., 1.8309e-03,\n",
       "          1.7429e-03, 1.6921e-03],\n",
       "         [1.6553e-03, 1.0901e-03, 9.6033e-04,  ..., 1.0790e-03,\n",
       "          1.6156e-03, 1.0462e-03],\n",
       "         ...,\n",
       "         [8.9108e-04, 7.3165e-04, 7.1886e-04,  ..., 5.5620e-04,\n",
       "          8.0379e-04, 5.4356e-04],\n",
       "         [1.0995e-03, 8.0209e-04, 1.3455e-03,  ..., 1.0828e-03,\n",
       "          1.2773e-03, 1.0145e-03],\n",
       "         [8.2158e-04, 6.2241e-04, 7.1242e-04,  ..., 3.9480e-04,\n",
       "          6.5885e-04, 6.5746e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[5.3367e-05, 5.2489e-05, 4.0843e-05,  ..., 3.6633e-05,\n",
       "          3.1191e-05, 4.6298e-05],\n",
       "         [8.4343e-04, 1.0552e-03, 6.1700e-04,  ..., 1.2287e-03,\n",
       "          5.8668e-04, 7.7857e-04],\n",
       "         [6.0673e-04, 5.8804e-04, 2.4586e-04,  ..., 5.7042e-04,\n",
       "          2.6697e-04, 4.0100e-04],\n",
       "         ...,\n",
       "         [1.1167e-03, 1.1394e-03, 1.0080e-03,  ..., 9.2189e-04,\n",
       "          1.0010e-03, 1.1988e-03],\n",
       "         [1.1207e-03, 1.1509e-03, 1.0650e-03,  ..., 9.8614e-04,\n",
       "          9.5520e-04, 1.1865e-03],\n",
       "         [3.3971e-04, 2.8165e-04, 2.7007e-04,  ..., 1.9378e-04,\n",
       "          2.1938e-04, 2.8442e-04]],\n",
       "\n",
       "        [[4.0742e-05, 4.3059e-05, 3.2720e-05,  ..., 2.9984e-05,\n",
       "          2.9082e-05, 3.9250e-05],\n",
       "         [6.1228e-04, 3.5819e-04, 6.1937e-04,  ..., 6.6856e-04,\n",
       "          6.1875e-04, 3.2287e-04],\n",
       "         [1.4965e-03, 1.0234e-03, 1.2705e-03,  ..., 1.5980e-03,\n",
       "          8.7222e-04, 6.8500e-04],\n",
       "         ...,\n",
       "         [1.0175e-03, 1.2438e-03, 1.4954e-03,  ..., 1.3014e-03,\n",
       "          8.0880e-04, 8.7571e-04],\n",
       "         [1.0185e-03, 1.0560e-03, 9.1295e-04,  ..., 1.3341e-03,\n",
       "          1.0315e-03, 1.1745e-03],\n",
       "         [2.8473e-04, 2.1105e-04, 2.2783e-04,  ..., 1.3314e-04,\n",
       "          1.7912e-04, 2.0923e-04]],\n",
       "\n",
       "        [[3.3988e-04, 3.6863e-04, 3.0503e-04,  ..., 2.8622e-04,\n",
       "          2.8624e-04, 2.7386e-04],\n",
       "         [1.9276e-04, 1.6600e-04, 1.3868e-04,  ..., 1.7835e-04,\n",
       "          1.1337e-04, 1.2327e-04],\n",
       "         [2.9524e-04, 1.8817e-04, 1.9592e-04,  ..., 3.8824e-04,\n",
       "          1.7117e-04, 2.7505e-04],\n",
       "         ...,\n",
       "         [1.2512e-03, 1.1111e-03, 1.2661e-03,  ..., 1.1421e-03,\n",
       "          1.1057e-03, 1.3125e-03],\n",
       "         [1.3333e-03, 1.1741e-03, 1.2455e-03,  ..., 1.2024e-03,\n",
       "          1.1003e-03, 1.2992e-03],\n",
       "         [1.3118e-03, 1.2496e-03, 1.3749e-03,  ..., 1.2365e-03,\n",
       "          1.2496e-03, 1.3232e-03]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(model_output[0], dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.0009, 0.0009,  ..., 0.0008, 0.0008, 0.0009],\n",
       "        [0.0015, 0.0016, 0.0013,  ..., 0.0011, 0.0012, 0.0010],\n",
       "        [0.0011, 0.0012, 0.0010,  ..., 0.0018, 0.0017, 0.0017],\n",
       "        ...,\n",
       "        [0.0008, 0.0011, 0.0006,  ..., 0.0012, 0.0006, 0.0008],\n",
       "        [0.0006, 0.0004, 0.0006,  ..., 0.0007, 0.0006, 0.0003],\n",
       "        [0.0002, 0.0002, 0.0001,  ..., 0.0002, 0.0001, 0.0001]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = probs[:,1]\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0013, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9998, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained BigBird tokenizer and model\n",
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base')\n",
    "\n",
    "# Load and preprocess the PDF document\n",
    "def preprocess_pdf_document(document_path):\n",
    "    text = \"\"\n",
    "    with open(document_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "document_path = \"./15031-4983-FullBook.pdf\"\n",
    "text = preprocess_pdf_document(document_path)\n",
    "\n",
    "# Tokenize and encode the document\n",
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "# Split the document into chunks of 512 tokens\n",
    "chunk_size = 512\n",
    "chunks = [input_ids[:, i:i+chunk_size] for i in range(0, input_ids.size(1), chunk_size)]\n",
    "chunk_attention_masks = [attention_mask[:, i:i+chunk_size] for i in range(0, attention_mask.size(1), chunk_size)]\n",
    "\n",
    "# Classify each chunk\n",
    "predicted_classes = []\n",
    "for chunk, attention_mask in zip(chunks, chunk_attention_masks):\n",
    "    outputs = model(chunk, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    print(\"Logits\",logits)\n",
    "    probabilities = logits.softmax(dim=1)\n",
    "    predicted_class = probabilities.argmax(dim=1).item()\n",
    "    predicted_classes.append(predicted_class)\n",
    "\n",
    "print(\"Predicted Classes:\", predicted_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
