{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_train.parquet')\n",
    "val_df = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_val.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base', \n",
    "                                                         num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label2id, tokenizer):\n",
    "        self.data = data.sample(4000)\n",
    "        self.label2id = label2id\n",
    "        self.tokenizer= tokenizer\n",
    "        self.labels = self.data[\"labels\"].apply(lambda x:self.label2id[x]).to_list()\n",
    "        self.one_hot_labels = pd.get_dummies(self.labels)\n",
    "        self.one_hot_labels = torch.tensor(np.array(self.one_hot_labels), dtype=torch.float)\n",
    "        print(\"Data:\",self.data.labels.value_counts())\n",
    "        print(\"counter:\",Counter(self.labels))\n",
    "        # print(\"@@@@\",len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index]['texts']\n",
    "        \n",
    "        # print(\"text::\",index,text)\n",
    "        # print('\\n')\n",
    "        label = self.one_hot_labels[index]\n",
    "        print(label)\n",
    "        # print(\"original length:\", len(text.split(\" \")), len(text))\n",
    "        encoded_text = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            max_length=512,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {'input_ids': encoded_text['input_ids'].squeeze(),\n",
    "                 'attention_mask': encoded_text['attention_mask'].squeeze(),\n",
    "                 'label':label}   \n",
    "                 \n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Level 2    1623\n",
      "Level 1    1249\n",
      "Level 3    1128\n",
      "Name: labels, dtype: int64\n",
      "counter: Counter({1: 1623, 0: 1249, 2: 1128})\n",
      "Data: Level 2    1398\n",
      "Level 3    1378\n",
      "Level 1    1224\n",
      "Name: labels, dtype: int64\n",
      "counter: Counter({1: 1398, 2: 1378, 0: 1224})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_data,label2id,tokenizer)\n",
    "test_dataset = CustomDataset(val_df,label2id,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=6,\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=False, \n",
    "                                           )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=6,\n",
    "                                           pin_memory=True,\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 0.0000025 ######\n",
    "BATCH_SIZE = 6\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.04) #####\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=0.04)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "             num_warmup_steps=50, ########\n",
    "            num_training_steps=len(train_loader)*EPOCHS )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711f135ccd93473380288bca8c65b267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 1., 0.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_num, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# tqdm_desc = f'Training ({step_num+1}/{train_loader_length})'\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(\"batch>>>\",batch_data)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     input_ids, att_mask, labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# input_ids, att_mask, labels = [data.to(device) for data in batch_data]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(input_ids \u001b[38;5;241m=\u001b[39m input_ids, attention_mask\u001b[38;5;241m=\u001b[39matt_mask, labels\u001b[38;5;241m=\u001b[39m labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_loader,desc='Training')):\n",
    "        # tqdm_desc = f'Training ({step_num+1}/{train_loader_length})'\n",
    "        # print(\"batch>>>\",batch_data)\n",
    "        input_ids, att_mask, labels = batch_data[\"input_ids\"].to(device),batch_data[\"attention_mask\"].to(device),batch_data[\"label\"].to(device)\n",
    "\n",
    "        # input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "        # print(\"logits***:\", output[\"logits\"])\n",
    "        \n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        del loss\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss / (step_num + 1))    \n",
    "\n",
    "\n",
    "    '''\n",
    "    Validation\n",
    "    '''\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(test_loader,desc='Validation')):\n",
    "            \n",
    "            input_ids, att_mask, labels = batch_data[\"input_ids\"].to(device),batch_data[\"attention_mask\"].to(device),batch_data[\"label\"].to(device)\n",
    "            # input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "            print(\"logits***:\", output[\"logits\"])\n",
    "\n",
    "            loss = output.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
    "    valid_pred = np.concatenate(valid_pred)\n",
    "\n",
    "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(val_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
    "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(val_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(epochs, train_loss_per_epoch, label='training loss')\n",
    "plt.plot(epochs, val_loss_per_epoch, label='validation loss')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('./loss_plot_big_bird.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_true = [batch[\"label\"].detach().cpu().numpy() for batch in test_loader]\n",
    "valid_true = np.concatenate(valid_true)\n",
    "valid_true = np.argmax(valid_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(valid_pred, valid_true, target_names= [\"level 1\",\"level 2\",\"level 3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BigBirdTokenizer,BigBirdModel,BigBirdForSequenceClassification\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = ''\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a document into smaller chunks\n",
    "def split_document(text, chunk_size):\n",
    "    # Split the text into chunks of size 'chunk_size'\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text of each document\n",
    "encoded_data = []\n",
    "text = extract_text_from_pdf(\"./15031-4983-FullBook.pdf\")\n",
    "encoded_text = tokenizer(text, padding='max_length', truncation=True, max_length=512)\n",
    "encoded_data.append({\n",
    "    'input_ids': encoded_text['input_ids'],\n",
    "    'attention_mask': encoded_text['attention_mask'],\n",
    "    'label': \"1\"  # Replace 'i' with the corresponding index or label for the document\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, add_special_tokens=True, truncation=False, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def split_overlapping(tensor: Tensor, chunk_size: int, stride: int, minimal_chunk_length: int) -> list[Tensor]:\n",
    "    \"\"\"Helper function for dividing 1-dimensional tensors into overlapping chunks.\"\"\"\n",
    "    result = [tensor[i : i + chunk_size] for i in range(0, len(tensor), stride)]\n",
    "    if len(result) > 1:\n",
    "        # ignore chunks with less than minimal_length number of tokens\n",
    "        result = [x for x in result if len(x) >= minimal_chunk_length]\n",
    "    return result\n",
    "example_tensor = tokens[\"input_ids\"][0]\n",
    "example_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
