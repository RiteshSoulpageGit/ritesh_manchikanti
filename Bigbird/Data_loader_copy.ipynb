{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BigBirdTokenizer,BigBirdModel,BigBirdForSequenceClassification\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# from typing import list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15031-4983-FullBook.docx</td>\n",
       "      <td>Learner Choice, Learning Voice\\n\\n\\nLearner Vo...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15031-4984-FullBook.docx</td>\n",
       "      <td>\\n\\nExistentialism: A Philosophical Inquiry\\n\\...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15031-4985-FullBook.docx</td>\n",
       "      <td>\"The editors of this volume are the top practi...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15031-4986-FullBook.docx</td>\n",
       "      <td>Black Power Music!\\n\\nBlack Power Music!: Prot...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15031-4989-FullBook.docx</td>\n",
       "      <td>Home\\n\\n\\nHome articulates a ‘critical geograp...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>15032-5774-FullBook.docx</td>\n",
       "      <td>The Japanese LGBTQ+ Community in the World\\n\\n...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>15032-5789-FullBook.docx</td>\n",
       "      <td>The Acquisition of English Grammar and Phonolo...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>15032-5829-FullBook.docx</td>\n",
       "      <td>\\n‘New concepts, new words for them, new actio...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>15032-5843-FullBook.docx</td>\n",
       "      <td>Translating Rumi into the West\\n\\n\\nFocusing o...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>15032-6039-FullBook.docx</td>\n",
       "      <td>Reassessing Vocational Education in China\\n\\nB...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0     15031-4983-FullBook.docx   \n",
       "1     15031-4984-FullBook.docx   \n",
       "2     15031-4985-FullBook.docx   \n",
       "3     15031-4986-FullBook.docx   \n",
       "4     15031-4989-FullBook.docx   \n",
       "...                        ...   \n",
       "1231  15032-5774-FullBook.docx   \n",
       "1232  15032-5789-FullBook.docx   \n",
       "1233  15032-5829-FullBook.docx   \n",
       "1234  15032-5843-FullBook.docx   \n",
       "1235  15032-6039-FullBook.docx   \n",
       "\n",
       "                                                   text    level  \n",
       "0     Learner Choice, Learning Voice\\n\\n\\nLearner Vo...  Level 1  \n",
       "1     \\n\\nExistentialism: A Philosophical Inquiry\\n\\...  Level 1  \n",
       "2     \"The editors of this volume are the top practi...  Level 1  \n",
       "3     Black Power Music!\\n\\nBlack Power Music!: Prot...  Level 1  \n",
       "4     Home\\n\\n\\nHome articulates a ‘critical geograp...  Level 1  \n",
       "...                                                 ...      ...  \n",
       "1231  The Japanese LGBTQ+ Community in the World\\n\\n...  Level 3  \n",
       "1232  The Acquisition of English Grammar and Phonolo...  Level 3  \n",
       "1233  \\n‘New concepts, new words for them, new actio...  Level 3  \n",
       "1234  Translating Rumi into the West\\n\\n\\nFocusing o...  Level 3  \n",
       "1235  Reassessing Vocational Education in China\\n\\nB...  Level 3  \n",
       "\n",
       "[1236 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data_csv = pd.read_csv(\"/home/ubuntu/working_directory/Bert_experimentation/new_dataframe_50000_100000.csv\")\n",
    "Data_csv = pd.read_csv(\"/home/ubuntu/cat_poc/llms/final_data.csv\")\n",
    "# Data_csv.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "Data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1    980\n",
      "Level 2    191\n",
      "Level 3     65\n",
      "Name: level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "level_counts = Data_csv['level'].value_counts()\n",
    "print(level_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = Data_csv.groupby(\"level\").apply(lambda x: x.sample(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Level 1</th>\n",
       "      <th>12</th>\n",
       "      <td>15031-5006-FullBook.docx</td>\n",
       "      <td>\"Kenneth Harrow’s book innovatively places sci...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15032-5263-FullBook.docx</td>\n",
       "      <td>Multidisciplinary Perspectives on Representati...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>15032-5692-FullBook.docx</td>\n",
       "      <td>Strategic Portfolio Management\\n\\nThis book pr...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>15031-5215-FullBook.docx</td>\n",
       "      <td>\\n\\nPromoting Your Voice on School Safety: \\nA...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>15032-5852-FullBook.docx</td>\n",
       "      <td>Making Democratic Theory Democratic\\nThis book...</td>\n",
       "      <td>Level 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Level 3</th>\n",
       "      <th>1210</th>\n",
       "      <td>15031-5420-FullBook.docx</td>\n",
       "      <td>A Dictionary of High Frequency Function Words ...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>15031-5074-FullBook.docx</td>\n",
       "      <td>The Social Archaeology of Late Second Temple J...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>15032-5447-FullBook.docx</td>\n",
       "      <td>Singapore Mandarin Grammar I\\n\\nAs the first v...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>15032-5474-FullBook.docx</td>\n",
       "      <td>\"This book takes a helpful evidence-based appr...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>15031-5541-FullBook.docx</td>\n",
       "      <td>&lt;&lt;halftitle page i – mandatory element&gt;&gt;\\nRout...</td>\n",
       "      <td>Level 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  \\\n",
       "level                                    \n",
       "Level 1 12    15031-5006-FullBook.docx   \n",
       "        397   15032-5263-FullBook.docx   \n",
       "        747   15032-5692-FullBook.docx   \n",
       "        132   15031-5215-FullBook.docx   \n",
       "        854   15032-5852-FullBook.docx   \n",
       "...                                ...   \n",
       "Level 3 1210  15031-5420-FullBook.docx   \n",
       "        1174  15031-5074-FullBook.docx   \n",
       "        1221  15032-5447-FullBook.docx   \n",
       "        1222  15032-5474-FullBook.docx   \n",
       "        1215  15031-5541-FullBook.docx   \n",
       "\n",
       "                                                           text    level  \n",
       "level                                                                     \n",
       "Level 1 12    \"Kenneth Harrow’s book innovatively places sci...  Level 1  \n",
       "        397   Multidisciplinary Perspectives on Representati...  Level 1  \n",
       "        747   Strategic Portfolio Management\\n\\nThis book pr...  Level 1  \n",
       "        132   \\n\\nPromoting Your Voice on School Safety: \\nA...  Level 1  \n",
       "        854   Making Democratic Theory Democratic\\nThis book...  Level 1  \n",
       "...                                                         ...      ...  \n",
       "Level 3 1210  A Dictionary of High Frequency Function Words ...  Level 3  \n",
       "        1174  The Social Archaeology of Late Second Temple J...  Level 3  \n",
       "        1221  Singapore Mandarin Grammar I\\n\\nAs the first v...  Level 3  \n",
       "        1222  \"This book takes a helpful evidence-based appr...  Level 3  \n",
       "        1215  <<halftitle page i – mandatory element>>\\nRout...  Level 3  \n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base', \n",
    "                                                         num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        # If adding the current sentence to the chunk will exceed the chunk length, start a new chunk\n",
    "        if current_length + sentence_length > chunk_length:\n",
    "            if current_length >= min_chunk_length:\n",
    "                # last_element = my_list[-1]\n",
    "                # sub_sentences = chunks[-1]\n",
    "                if chunks:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", chunks[-1])\n",
    "                else:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "\n",
    "                sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "                for a, sub_sentence in enumerate(sub_sentences):\n",
    "                    sub_sentence_length = len(sub_sentence)\n",
    "                    if current_length + sub_sentence_length > chunk_length:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = \"\"\n",
    "                        current_length = 0\n",
    "                    current_chunk += sub_sentence + \" \"\n",
    "                    current_length += sub_sentence_length\n",
    "                    # current_length += len(sentence)\n",
    "                    # if current_length >= stride:\n",
    "                    #     remove_index = a + 1\n",
    "                    #     break\n",
    "\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "                current_length = 0\n",
    "        current_chunk += sentence + \" \"\n",
    "        current_length += sentence_length\n",
    "    if current_length >= min_chunk_length:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = Data_csv.loc[1171]['text'] \n",
    "characters = [\".\", \"?\", \"!\"]\n",
    "character_indices = [i for i, char in enumerate(long_string) if char in characters]\n",
    "\n",
    "print(len(character_indices))\n",
    "sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", long_string)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n",
    "for idx in range(sample_df.shape[0]):\n",
    "    x = sample_df[\"text\"].iloc[idx]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', x)\n",
    "    long_string = ' '.join(sentences)\n",
    "    label = label2id[sample_df.iloc[idx][\"level\"]]\n",
    "    chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "    trimmed_chunks = [string[:512] for string in chunks]\n",
    "    for i in trimmed_chunks:\n",
    "        if len(i) > 512:\n",
    "            print(len(i))\n",
    "    # print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = Data_csv.loc[1171]['text']  # Your long string\n",
    "print(len(long_string))\n",
    "stride = 386 # Stride value\n",
    "chunk_length = 510  # Maximum chunk length\n",
    "min_chunk_length = 250 # Minimum chunk length\n",
    "chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "# new_chunks = build_overlapping_chunks(chunks, chunk_length, stride)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    chunks = []\n",
    "    current_chunk = \" \"\n",
    "    for sentence in sentences:\n",
    "        sen_len = len(sentence.split())\n",
    "        curr_chunk_len = len(current_chunk.split())\n",
    "        if sen_len + curr_chunk_len > 400:\n",
    "            chunks.append(current_chunk)\n",
    "            prev_chunk = chunks[-1]\n",
    "            sub_sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", prev_chunk)\n",
    "            temp_buffer = \" \"\n",
    "            for sub_sentece in reversed(sub_sentences):\n",
    "                sub_sen_len = len(sub_sentece.split())\n",
    "                temp_buffer_len = len(temp_buffer.split())\n",
    "                if temp_buffer_len + sub_sen_len > 128:\n",
    "                    current_chunk = temp_buffer\n",
    "                    break\n",
    "                temp_buffer = sub_sentece + temp_buffer\n",
    "            # current_chunk += temp_buffer\n",
    "        current_chunk += sentence\n",
    "    return chunks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = Data_csv.loc[1171]['text'] \n",
    "# sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", long_string)\n",
    "# print(len(sentences))\n",
    "chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "print(chunks[0])\n",
    "print(\"__________\")\n",
    "print(chunks[1])\n",
    "print(\"__________\")\n",
    "print(chunks[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = Data_csv.loc[1171]['text'] \n",
    "# sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", long_string)\n",
    "# print(len(sentences))\n",
    "chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "\n",
    "tokenized_chunks = []\n",
    "for chunk in chunks[0:1]:\n",
    "    print(\"chunk_length\",len(chunk.split()))\n",
    "    tokens = tokenizer(chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    print(len(tokens['input_ids'][0]))\n",
    "    print(chunk)\n",
    "    print(tokens['input_ids'])\n",
    "    tokenized_chunks.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    chunks = []\n",
    "    current_chunk = \" \"\n",
    "    for sentence in sentences:\n",
    "        sen_len = len(sentence)\n",
    "        curr_chunk_len = len(current_chunk)\n",
    "        if sen_len + curr_chunk_len > chunk_length:\n",
    "            chunks.append(current_chunk)\n",
    "            prev_chunk = chunks[-1]\n",
    "            sub_sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", prev_chunk)\n",
    "            temp_buffer = \" \"\n",
    "            for sub_sentece in reversed(sub_sentences):\n",
    "                sub_sen_len = len(sub_sentece)\n",
    "                temp_buffer_len = len(temp_buffer)\n",
    "                if temp_buffer_len + sub_sen_len > stride:\n",
    "                    current_chunk = temp_buffer\n",
    "                    break\n",
    "                temp_buffer = sub_sentece + temp_buffer\n",
    "            # current_chunk += temp_buffer\n",
    "        current_chunk += sentence\n",
    "    return chunks\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!|\\,|\\;|\\n)\\s\", text)\n",
    "    chunks = []\n",
    "    current_chunk = \" \"\n",
    "    for sentence in sentences:\n",
    "        sen_len = len(sentence.split())\n",
    "        curr_chunk_len = len(current_chunk.split())\n",
    "        if sen_len + curr_chunk_len > 250:\n",
    "            chunks.append(current_chunk)\n",
    "            prev_chunk = chunks[-1]\n",
    "            sub_sentences = re.split(r\"(?<=\\.|\\?|\\!|\\,|\\;|\\n)\\s\", prev_chunk)\n",
    "            temp_buffer = \" \"\n",
    "            for sub_sentece in reversed(sub_sentences):\n",
    "                sub_sen_len = len(sub_sentece.split())\n",
    "                temp_buffer_len = len(temp_buffer.split())\n",
    "                if temp_buffer_len + sub_sen_len > 64:\n",
    "                    current_chunk = temp_buffer\n",
    "                    break\n",
    "                temp_buffer = sub_sentece + temp_buffer\n",
    "            # current_chunk += temp_buffer\n",
    "        current_chunk += sentence\n",
    "    return chunks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 256,\n",
    "    length_function = len,\n",
    ")\n",
    "long_string = Data_csv.loc[1171]['text'] \n",
    "\n",
    "texts = text_splitter.create_documents([long_string])\n",
    "print(texts[0])\n",
    "print(\"------------\")\n",
    "print(texts[1])\n",
    "print(\"------------\")\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter_1 = TokenTextSplitter(chunk_size=300, chunk_overlap=128)\n",
    "\n",
    "text_splitter_2 = TokenTextSplitter(chunk_size=150, chunk_overlap=64)\n",
    "\n",
    "long_string = Data_csv.loc[1171]['text'] \n",
    "texts = text_splitter.split_text(long_string)\n",
    "# print(texts[0])\n",
    "# print(\"------------\")\n",
    "# print(texts[1])\n",
    "# print(\"------------\")\n",
    "# print(texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_into_smaller_chunks(input_id: Tensor,att_mask: Tensor, chunk_size: int, stride: int, minimal_chunk_length: int):\n",
    "    input_id_chunks = [input_id[i : i + chunk_size] for i in range(0, len(input_id), stride)]\n",
    "    mask_chunks = [att_mask[i : i + chunk_size] for i in range(0, len(att_mask), stride)]\n",
    "    if len(input_id_chunks) > 1:\n",
    "        # ignore chunks with less than minimal_length number of tokens\n",
    "        input_id_chunks = [x for x in input_id_chunks if len(x) >= minimal_chunk_length]\n",
    "        mask_chunks = [x for x in mask_chunks if len(x) >= minimal_chunk_length]\n",
    "    return input_id_chunks, mask_chunks\n",
    "\n",
    "# from typing import List\n",
    "\n",
    "# def split_tokens_into_smaller_chunks(input_ids: List[int], attention_mask: List[int], chunk_size: int, stride: int, minimal_chunk_length: int):\n",
    "#     input_id_chunks = [input_ids[i : i + chunk_size] for i in range(0, len(input_ids), stride)]\n",
    "#     mask_chunks = [attention_mask[i : i + chunk_size] for i in range(0, len(attention_mask), stride)]\n",
    "#     if len(input_id_chunks) > 1:\n",
    "#         # ignore chunks with less than minimal_length number of tokens\n",
    "#         input_id_chunks = [x for x in input_id_chunks if len(x) >= minimal_chunk_length]\n",
    "#         mask_chunks = [x for x in mask_chunks if len(x) >= minimal_chunk_length]\n",
    "#     return input_id_chunks, mask_chunks\n",
    "\n",
    "def add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks) -> None:\n",
    "    \"\"\"\n",
    "    Adds special CLS token (token id = 101) at the beginning.\n",
    "    Adds SEP token (token id = 102) at the end of each chunk.\n",
    "    Adds corresponding attention masks equal to 1 (attention mask is boolean).\n",
    "    \"\"\"\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # adding CLS (token id 101) and SEP (token id 102) tokens\n",
    "        input_id_chunks[i] = torch.cat([Tensor([101]), input_id_chunks[i], Tensor([102])])\n",
    "        # adding attention masks  corresponding to special tokens\n",
    "        mask_chunks[i] = torch.cat([Tensor([1]), mask_chunks[i], Tensor([1])])\n",
    "\n",
    "def add_padding_tokens(input_id_chunks, mask_chunks) -> None:\n",
    "    \"\"\"Adds padding tokens (token id = 0) at the end to make sure that all chunks have exactly 512 tokens.\"\"\"\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # get required padding length\n",
    "        pad_len = 512 - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([input_id_chunks[i], Tensor([0] * pad_len)])\n",
    "            mask_chunks[i] = torch.cat([mask_chunks[i], Tensor([0] * pad_len)])\n",
    "\n",
    "# def add_padding_tokens(input_id_chunks, mask_chunks) -> None:\n",
    "#     \"\"\"Adds padding tokens (token id = 0) at the end to make sure that all chunks have exactly 512 tokens.\"\"\"\n",
    "#     for i in range(len(input_id_chunks)):\n",
    "#         # get required padding length\n",
    "#         pad_len = 512 - len(input_id_chunks[i])\n",
    "#         # check if list length satisfies required chunk size\n",
    "#         if pad_len > 0:\n",
    "#             # if padding length is more than 0, we must add padding\n",
    "#             input_id_chunks[i] += [0] * pad_len\n",
    "#             mask_chunks[i] += [0] * pad_len\n",
    "\n",
    "def stack_tokens_from_all_chunks(input_id_chunks, mask_chunks):\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "    return input_ids.long(), attention_mask.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter_1 = TokenTextSplitter(chunk_size=300, chunk_overlap=128)\n",
    "\n",
    "text_splitter_2 = TokenTextSplitter(chunk_size=150, chunk_overlap=64)\n",
    "\n",
    "new_chunk_df = pd.DataFrame()\n",
    "total_chunks = []  \n",
    "total_att_mask = [] \n",
    "total_fnames = []\n",
    "total_labels = []\n",
    "total_chunk_counts = []\n",
    "\n",
    "label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n",
    "chunks_count = 1\n",
    "chunk_length = 2000\n",
    "stride = 500\n",
    "min_chunk_length = 256\n",
    "length_total_labels = 0\n",
    "\n",
    "chunk_len_list = []\n",
    "token_len_list = []\n",
    "for idx in range(sample_df.shape[0]):\n",
    "# for idx in range(2):\n",
    "    x = sample_df[\"text\"].iloc[idx]\n",
    "    # sentences = re.split(r'(?<=[.!?])\\s+', x)\n",
    "    # long_string = ' '.join(sentences)\n",
    "    label = label2id[sample_df.iloc[idx][\"level\"]]\n",
    "    # chunks = split_text_into_chunks(x, stride, chunk_length, min_chunk_length)\n",
    "    chunks = text_splitter_1.split_text(x)\n",
    "    # length_total_labels += len(chunks)\n",
    "    tokenized_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # print(\"chunk_length\",len(chunk.split()))\n",
    "        chunk_len_list.append(len(chunk.split()))\n",
    "        \n",
    "        tokens = tokenizer(chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        if len(tokens['input_ids'][0]) > 512:\n",
    "            sub_chunks = text_splitter_2.split_text(chunk)\n",
    "            print(len(sub_chunks))\n",
    "            for i in sub_chunks:\n",
    "                tokens = tokenizer(chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "                tokenized_chunks.append(tokens)\n",
    "            # print(len(tokens['input_ids'][0]))\n",
    "        else:\n",
    "            token_len_list.append(len(tokens['input_ids'][0]))\n",
    "            tokenized_chunks.append(tokens)\n",
    "    input_id_chunks = []\n",
    "    mask_chunks = []\n",
    "    for chunk in tokenized_chunks:\n",
    "        input_id_chunks.extend(chunk[\"input_ids\"])\n",
    "        mask_chunks.extend(chunk[\"attention_mask\"])\n",
    "\n",
    "    add_padding_tokens(input_id_chunks, mask_chunks)\n",
    "    label_vec = [label] * len(input_id_chunks)\n",
    "    total_chunks.extend((input_id_chunks))\n",
    "    total_att_mask.extend((mask_chunks))\n",
    "    total_labels.extend((label_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in total_chunks:\n",
    "    if len(i) > 512:\n",
    "        print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels_vec):\n",
    "        self.input_ids = [chunk.long() for chunk in input_ids]\n",
    "        # print(\"length in put ids:\", len(self.input_ids))\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels_vec = labels_vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(self.labels_vec[index])\n",
    "        return {\n",
    "            'input_ids': self.input_ids[index].squeeze(),\n",
    "            'attention_mask': self.attention_mask[index].squeeze(),\n",
    "            'label': self.labels_vec[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(total_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  CustomDataset(total_chunks,total_att_mask,total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset[0]['input_ids'])\n",
    "for i in dataset:\n",
    "    if len(i['input_ids']) != 512: \n",
    "        print(len(i['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_labels.count(0))\n",
    "print(total_labels.count(1))\n",
    "print(total_labels.count(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_ratio = 0.8  # 80% for training\n",
    "test_ratio = 0.2  # 20% for testing\n",
    "\n",
    "num_samples = len(dataset)\n",
    "train_size = int(train_ratio * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset[3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_train.parquet')\n",
    "# val_df = pd.read_parquet('/home/ubuntu/working_directory/Bert_experimentation/80_val.parquet')\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data, label2id, tokenizer):\n",
    "#         self.data = data.sample(4000)\n",
    "#         self.label2id = label2id\n",
    "#         self.tokenizer= tokenizer\n",
    "#         self.labels = self.data[\"labels\"].apply(lambda x:self.label2id[x]).to_list()\n",
    "#         self.one_hot_labels = pd.get_dummies(self.labels)\n",
    "#         self.one_hot_labels = torch.tensor(np.array(self.one_hot_labels), dtype=torch.float)\n",
    "#         print(\"Data:\",self.data.labels.value_counts())\n",
    "#         print(\"counter:\",Counter(self.labels))\n",
    "#         # print(\"@@@@\",len(self.data))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         text = self.data.iloc[index]['texts']\n",
    "\n",
    "#         # print(\"text::\",index,text)\n",
    "#         # print('\\n')\n",
    "#         label = self.one_hot_labels[index]\n",
    "#         # print(\"original length:\", len(text.split(\" \")), len(text))\n",
    "#         encoded_text = self.tokenizer.encode_plus(\n",
    "#             str(text),\n",
    "#             max_length=512,\n",
    "#             add_special_tokens=True,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {'input_ids': encoded_text['input_ids'].squeeze(),\n",
    "#                  'attention_mask': encoded_text['attention_mask'].squeeze(),\n",
    "#                  'label':label}   \n",
    "                 \n",
    "                 \n",
    "# label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n",
    "# old_train_dataset = CustomDataset(train_data,label2id,tokenizer)\n",
    "# old_test_dataset = CustomDataset(val_df,label2id,tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(old_train_dataset)\n",
    "# print(old_test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=5,\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=True, \n",
    "                                           )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=5,\n",
    "                                           pin_memory=True,\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0000025 ######\n",
    "BATCH_SIZE = 5\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.04) #####\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=0.04)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "             num_warmup_steps=50, ########\n",
    "            num_training_steps=len(train_loader)*EPOCHS )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_loader,desc='Training')):\n",
    "        # tqdm_desc = f'Training ({step_num+1}/{train_loader_length})'\n",
    "        # print(\"batch>>>\",batch_data)\n",
    "        input_ids, att_mask, labels = batch_data[\"input_ids\"].to(device),batch_data[\"attention_mask\"].to(device),batch_data[\"label\"].to(device)\n",
    "        input_ids = input_ids.to(torch.long).to(device)\n",
    "        att_mask = att_mask.to(torch.long).to(device)\n",
    "        labels = labels.to(torch.long).to(device)\n",
    "        # print(input_ids.shape,labels.shape)\n",
    "        # input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "        # print(\"logits***:\", output[\"logits\"])\n",
    "        \n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        del loss\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss / (step_num + 1))    \n",
    "\n",
    "\n",
    "    '''\n",
    "    Validation\n",
    "    '''\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(test_loader,desc='Validation')):\n",
    "            \n",
    "            input_ids, att_mask, labels = batch_data[\"input_ids\"].to(device),batch_data[\"attention_mask\"].to(device),batch_data[\"label\"].to(device)\n",
    "            # input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "            # print(\"logits***:\", output[\"logits\"])\n",
    "\n",
    "            loss = output.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
    "    valid_pred = np.concatenate(valid_pred)\n",
    "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_dataset) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
    "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(test_dataset) / BATCH_SIZE), valid_loss / (step_num_e + 1)))          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(epochs, train_loss_per_epoch, label='training loss')\n",
    "plt.plot(epochs, val_loss_per_epoch, label='validation loss')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('./loss_plotBig_bird_new_data.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_true = [batch[\"label\"].detach().cpu().numpy() for batch in test_loader]\n",
    "valid_true = np.concatenate(valid_true)\n",
    "# valid_true = np.argmax(valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(valid_pred, valid_true,labels=[0,1,2], target_names= [\"level 1\",\"level 2\",\"level 3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./savedmodel_2\")\n",
    "tokenizer.save_pretrained(\"./savedmodel_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdForSequenceClassification, BigBirdTokenizer\n",
    "# Load the saved model and tokenizer\n",
    "model = BigBirdForSequenceClassification.from_pretrained(\"./savedmodel_2/\")\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(\"./savedmodel_2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = ''\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "level_3_rows = sample_df.loc[sample_df['Level'] == 'Level 1']\n",
    "print(len(level_3_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_3_rows = Data_csv.loc[Data_csv['Level'] == 'Level 3']\n",
    "print(len(level_3_rows['FileName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data= pd.read_csv(\"/home/ubuntu/cat_poc/llms/data.csv\")\n",
    "original_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['CE_Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store the predicted labels for each row\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for index, row in Data_csv.iterrows():\n",
    "    print(index)\n",
    "    document = row['text']\n",
    "    total_words = len(document)\n",
    "    chunk_size = 512\n",
    "    overlap = 256\n",
    "    total_chunks = (total_words - chunk_size) // (chunk_size - overlap) + 1\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(total_chunks):\n",
    "        start_index = i * (chunk_size - overlap)\n",
    "        end_index = start_index + chunk_size\n",
    "        chunk = document[start_index:end_index]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    tokenized_chunks = []\n",
    "    for chunk in chunks:\n",
    "        tokens = tokenizer(chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        tokenized_chunks.append(tokens)\n",
    "\n",
    "    chunk_predicted_labels = []\n",
    "    for tokens in tokenized_chunks:\n",
    "        outputs = model(**tokens)\n",
    "        # Get the predicted class index\n",
    "        predicted_class_index = outputs.logits.argmax().item()\n",
    "        # Get the predicted class label\n",
    "        label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n",
    "        id_to_label = {v: k for k, v in label2id.items()}\n",
    "        predicted_class_label = id_to_label[predicted_class_index]\n",
    "        \n",
    "        chunk_predicted_labels.append(predicted_class_label)\n",
    "\n",
    "    # Find the majority predicted label for all the chunks\n",
    "    majority_label = max(set(chunk_predicted_labels), key=chunk_predicted_labels.count)\n",
    "    predicted_labels.append(majority_label)\n",
    "    print(\"True label vs predicted label\",row['level'],predicted_class_label)\n",
    "\n",
    "# Count the occurrence of each predicted label\n",
    "label_counts = pd.Series(predicted_labels).value_counts()\n",
    "\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# level_3_rows = Data_csv.loc[Data_csv['Level'] == 'Level 2']\n",
    "# row = level_3_rows.iloc[10]\n",
    "# Random document\n",
    "# document = extract_text_from_pdf(\"./15031-4983-FullBook.pdf\")\n",
    "print(row)\n",
    "document = row['text']\n",
    "print(len(document.split()))\n",
    "# Tokenize the document\n",
    "tokens = tokenizer(document, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "print(tokens['input_ids'].shape)\n",
    "\n",
    "# input_id_chunks, mask_chunks = split_tokens_into_smaller_chunks(tokens[\"input_ids\"][0],tokens[\"attention_mask\"][0], chunk_size, stride, minimal_chunk_length)\n",
    "# add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n",
    "# add_padding_tokens(input_id_chunks, mask_chunks)\n",
    "\n",
    "# # Forward pass through the model\n",
    "# outputs = model(**tokens)\n",
    "\n",
    "# # Get the predicted class index\n",
    "# predicted_class_index = outputs.logits.argmax().item()\n",
    "\n",
    "# # Get the predicted class label\n",
    "# # label2id = model.config.id2label\n",
    "# label2id = {\"Level 1\": 0, \"Level 2\": 1, \"Level 3\": 2}\n",
    "# id_to_label = {v: k for k, v in label2id.items()}\n",
    "# predicted_class_label = id_to_label[predicted_class_index]\n",
    "\n",
    "# print(\"Predicted Class:\", predicted_class_label)\n",
    "\n",
    "total_words = len(document)\n",
    "chunk_size = 512\n",
    "overlap = 256\n",
    "total_chunks = (total_words - chunk_size) // (chunk_size - overlap) + 1\n",
    "\n",
    "chunks = []\n",
    "for i in range(total_chunks):\n",
    "    start_index = i * (chunk_size - overlap)\n",
    "    end_index = start_index + chunk_size\n",
    "    chunk = document[start_index:end_index]\n",
    "    chunks.append(chunk)\n",
    "tokenized_chunks = []\n",
    "for chunk in chunks:\n",
    "    tokens = tokenizer(chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    tokenized_chunks.append(tokens)\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for i in tokenized_chunks:\n",
    "    outputs = model(**tokens)\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = outputs.logits.argmax().item()\n",
    "    # Get the predicted class label\n",
    "    predicted_class_label = id_to_label[predicted_class_index]\n",
    "    predicted_labels.append(predicted_class_label) \n",
    "\n",
    "print(len(predicted_labels))\n",
    "print(predicted_labels.count('Level 1'))\n",
    "print(predicted_labels.count('Level 2'))\n",
    "print(predicted_labels.count('Level 3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the rows in the new DataFrame\n",
    "for index, row in new_df.iterrows():\n",
    "    document = row['text']\n",
    "    print(len(document))\n",
    "    true_label = row['Level']\n",
    "    true_labels.append(true_label)\n",
    "    # Tokenize the document\n",
    "    tokens = tokenizer(document, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    print(len(tokens))\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**tokens)\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = outputs.logits.argmax().item()\n",
    "    # Get the predicted class label\n",
    "    predicted_class_label = id_to_label[predicted_class_index]\n",
    "    predicted_labels.append(predicted_class_label)\n",
    "\n",
    "document_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Overall Accuracy:\", document_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_labels)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_1_true_label = true_label[:5]\n",
    "lv_2_true_label = true_label[5:10]\n",
    "lv_3_true_label = true_label[10:]\n",
    "\n",
    "\n",
    "lv_1_pred_label = predicted_labels[:5]\n",
    "lv_2_pred_label = predicted_labels[5:10]\n",
    "lv_3_pred_label = predicted_labels[10:]\n",
    "print(\"level - 1 accuracy\",accuracy_score(lv_1_true_label,lv_1_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
