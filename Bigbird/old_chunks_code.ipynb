{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        # If adding the current sentence to the chunk will exceed the chunk length, start a new chunk\n",
    "        if current_length + sentence_length > chunk_length:\n",
    "            if current_length >= min_chunk_length:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "                current_length = 0\n",
    "        current_chunk += sentence + \" \"\n",
    "        current_length += sentence_length\n",
    "    if current_length >= min_chunk_length:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def build_overlapping_chunks(elements, max_length, overlap):\n",
    "    chunks = []\n",
    "    previous_chunk = elements[0]\n",
    "    chunks.append(previous_chunk)\n",
    "    for i in range(1, len(elements)):\n",
    "        current_chunk = elements[i]\n",
    "\n",
    "        # Combine previous chunk and current chunk\n",
    "        combined_chunk = previous_chunk + ' ' + current_chunk\n",
    "\n",
    "        if len(combined_chunk) <= max_length:\n",
    "            # If the combined chunk is within the maximum length, set it as the current chunk\n",
    "            current_chunk = combined_chunk\n",
    "        else:\n",
    "            # If the combined chunk exceeds the maximum length, remove sentences from previous chunk\n",
    "            sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", previous_chunk)\n",
    "            \n",
    "            # remaining_length = max_length - len(current_chunk)\n",
    "\n",
    "            # Add sentences from previous chunk until remaining length is reached\n",
    "            for sentence in reversed(sentences):\n",
    "                # print(len(sentences))\n",
    "                if len(current_chunk) + len(sentence) <= max_length:\n",
    "                    # print(\"adding\")\n",
    "                    current_chunk = sentence + '. ' + current_chunk \n",
    "                    # remaining_length -= len(sentence)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Trim excess characters from current chunk\n",
    "            # current_chunk = current_chunk[:max_length]\n",
    "\n",
    "        # Add current chunk to the list of chunks\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "        # Update previous chunk for the next iteration\n",
    "        previous_chunk = current_chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        # If adding the current sentence to the chunk will exceed the chunk length, start a new chunk\n",
    "        if current_length + sentence_length > chunk_length:\n",
    "            if current_length >= min_chunk_length:\n",
    "                # last_element = my_list[-1]\n",
    "                # sub_sentences = chunks[-1]\n",
    "                if chunks:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", chunks[-1])\n",
    "                else:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "\n",
    "                sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "                for a, sub_sentence in enumerate(sub_sentences):\n",
    "                    sub_sentence_length = len(sub_sentence)\n",
    "                    if current_length + sub_sentence_length > chunk_length:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = \"\"\n",
    "                        current_length = 0\n",
    "                    current_chunk += sub_sentence + \" \"\n",
    "                    current_length += sub_sentence_length\n",
    "                    # current_length += len(sentence)\n",
    "                    # if current_length >= stride:\n",
    "                    #     remove_index = a + 1\n",
    "                    #     break\n",
    "\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "                current_length = 0\n",
    "        current_chunk += sentence + \" \"\n",
    "        current_length += sentence_length\n",
    "    if current_length >= min_chunk_length:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m long_string \u001b[38;5;241m=\u001b[39m \u001b[43mData_csv\u001b[49m\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m1171\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Your long string\u001b[39;00m\n\u001b[1;32m      2\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;66;03m# Stride value\u001b[39;00m\n\u001b[1;32m      3\u001b[0m chunk_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m510\u001b[39m  \u001b[38;5;66;03m# Maximum chunk length\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data_csv' is not defined"
     ]
    }
   ],
   "source": [
    "long_string = Data_csv.loc[1171]['text']  # Your long string\n",
    "stride = 256 # Stride value\n",
    "chunk_length = 510  # Maximum chunk length\n",
    "min_chunk_length = 1  # Minimum chunk length\n",
    "chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "# new_chunks = build_overlapping_chunks(chunks, chunk_length, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_into_chunks(text, stride, chunk_length, min_chunk_length):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        # If adding the current sentence to the chunk will exceed the chunk length, start a new chunk\n",
    "        if current_length + sentence_length > chunk_length:\n",
    "            if current_length >= min_chunk_length:\n",
    "                # last_element = my_list[-1]\n",
    "                # sub_sentences = chunks[-1]\n",
    "                if chunks:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", chunks[-1])\n",
    "                else:\n",
    "                    sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "\n",
    "                sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "                for a, sub_sentence in enumerate(sub_sentences):\n",
    "                    sub_sentence_length = len(sub_sentence)\n",
    "                    if current_length + sub_sentence_length > chunk_length:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = \"\"\n",
    "                        current_length = 0\n",
    "                    current_chunk += sub_sentence + \" \"\n",
    "                    current_length += sub_sentence_length\n",
    "                    # current_length += len(sentence)\n",
    "                    # if current_length >= stride:\n",
    "                    #     remove_index = a + 1\n",
    "                    #     break\n",
    "\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "                current_length = 0\n",
    "        current_chunk += sentence + \" \"\n",
    "        current_length += sentence_length\n",
    "    if current_length >= min_chunk_length:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def split_text_into_chunks_two(text, stride, chunk_length, min_chunk_length):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|\\!)\\s\", text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    i = 0 \n",
    "    while sentences:\n",
    "        # print(\"len= \",len(sentences))\n",
    "        i += 1\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) > chunk_length:\n",
    "                sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "                for sub_sentence in sub_sentences:\n",
    "                    if len(current_chunk) + len(sub_sentence) > chunk_length:\n",
    "                        chunks.append(current_chunk)\n",
    "                        # print(\"len- chunk - \",len(current_chunk))\n",
    "                        current_chunk = \"\"\n",
    "                    current_chunk += sub_sentence + \" \"        \n",
    "            current_chunk += sentence + \" \"\n",
    "            # sentences.remove(sentence)\n",
    "\n",
    "        if len(current_chunk) >= min_chunk_length:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        current_length = 0\n",
    "        remove_index = 0\n",
    "        for a, sentence in enumerate(sentences):\n",
    "            current_length += len(sentence)\n",
    "            if current_length >= stride:\n",
    "                remove_index = a + 1\n",
    "                break\n",
    "        sentences = sentences[remove_index:]\n",
    "        if i == 5000:\n",
    "            break\n",
    "    return chunks        \n",
    "            # sub_sentences = re.split(r\"(?<=\\;|\\,|\\.|\\?|\\!)\\s\", sentence)\n",
    "            # removable_chunk = []\n",
    "            # for sub_sentence in sub_senteces:\n",
    "            #     if len(removable_chunk) + len(sub_sentence) > stride:\n",
    "            #         sentences = sentences[sentences.index(sub_sentence):]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = Data_csv.loc[1171]['text']  # Your long string\n",
    "print(len(long_string))\n",
    "stride = 386 # Stride value\n",
    "chunk_length = 510  # Maximum chunk length\n",
    "min_chunk_length = 250 # Minimum chunk length\n",
    "chunks = split_text_into_chunks(long_string, stride, chunk_length, min_chunk_length)\n",
    "# new_chunks = build_overlapping_chunks(chunks, chunk_length, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "# print(chunks[0])\n",
    "# print(\"----\")\n",
    "# print(chunks[1])\n",
    "# print(\"----\")\n",
    "# print(chunks[2])\n",
    "# print(\"----\")\n",
    "# print(chunks[3])\n",
    "# print(\"----\")\n",
    "# print(chunks[4])\n",
    "# print(\"----\")\n",
    "# print(chunks[5])\n",
    "# print(\"----\")\n",
    "# print(chunks[6])\n",
    "# print(\"----\")\n",
    "# print(chunks[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_levels = Data_csv['level'].unique()\n",
    "new_df = pd.DataFrame(columns=Data_csv.columns)\n",
    "for level in unique_levels:\n",
    "    level_docs = Data_csv[Data_csv['level'] == level]\n",
    "    level_docs_not_in_sample = level_docs[~level_docs.index.isin(sample_df.index)]\n",
    "    selected_docs = level_docs_not_in_sample.sample(5)\n",
    "    new_df = new_df.append(selected_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # input_id_chunks, mask_chunks = tokens[\"input_ids\"][0], tokens[\"attention_mask\"][0]\n",
    "    # add_special_tokens_at_beginning_and_end(tokens[\"input_ids\"][0], tokens[\"attention_mask\"][0])\n",
    "    # add_padding_tokens(tokens[\"input_ids\"][0], tokens[\"attention_mask\"][0])\n",
    "    # label_vec = [label] * len(tokens[\"input_ids\"][0])\n",
    "    # total_chunks.extend((input_id_chunks))\n",
    "    # total_att_mask.extend((mask_chunks))\n",
    "    # total_labels.extend((label_vec))\n",
    "    # tokens = tokenizer(long_string, add_special_tokens=True, truncation=False, return_tensors=\"pt\")\n",
    "    # input_id_chunks, mask_chunks = split_tokens_into_smaller_chunks(input_id_chunks,mask_chunks, 512, 128, min_chunk_length)\n",
    "    # add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n",
    "    # label_vec = [label] * len(padded_input_id_chunks)\n",
    "    # total_chunks.extend((padded_input_id_chunks))\n",
    "    # total_att_mask.extend((padded_mask_chunks)) \n",
    "    # total_labels.extend((label_vec))\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
